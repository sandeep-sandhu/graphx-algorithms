{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Graph Algorithms - Part 1 - The Basics\n",
    "\n",
    "Algorithms implemented in the Apache Spark GraphX platform.\n",
    "\n",
    "The following topics have been covered in this notebook:\n",
    "\n",
    "  1. Initialising the environment\n",
    "  1. Starting a Spark session\n",
    "  1. Load sample graph data\n",
    "  1. Creating graphs from datasets\n",
    "  1. Generating artifical graphs\n",
    "  1. Basic GraphX properties\n",
    "  1. Updating Edge data of a graph\n",
    "  1. Updating vertex data of a graph\n",
    "  1. Extracting vertex data from graph\n",
    "  2. Saving graph to JSON format\n",
    "  3. Saving graph to GEXF (Gephi) format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Initialising the environment\n",
    "\n",
    "### 1.1 Source the libraries for Apache Spark\n",
    "\n",
    "When running in a jupyter notebook, sometimes the required libraries may not exist in the classpath.\n",
    "\n",
    "Load essential spark libraries from maven public repositories at runtime like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "a7ca08d823f240129c8c3b8e1deb4fa2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                          \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                        \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-core:3.4.0`\n",
    "import $ivy.`org.apache.spark::spark-mllib-local:3.4.0`\n",
    "import $ivy.`org.apache.spark::spark-mllib:3.4.0`\n",
    "import $ivy.`org.apache.spark::spark-graphx:3.4.0`\n",
    "import $ivy.`org.apache.spark::spark-streaming:3.4.0`\n",
    "import $ivy.`org.apache.spark::spark-tags:3.4.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                   \u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.scalanlp::breeze-viz:2.1.0`\n",
    "import $ivy.`org.jfree:jfreechart:1.5.4`\n",
    "import $ivy.`org.creativescala::doodle-core:0.18.0`\n",
    "import $ivy.`com.fasterxml.jackson.core:jackson-databind:2.15.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.2 Import the Spark Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "732e8d55868d4b848e8a1aa50e5a9a12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.linalg.{Matrix, Vectors}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Row\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Dataset\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.{col, udf, _}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.graphx._\n",
       "// To make some of the examples work we will also need RDD\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.rdd.RDD\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.storage.StorageLevel\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.graphx.util.GraphGenerators\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.linalg.{Matrix, Vectors}\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.functions.{col, udf, _}\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.graphx._\n",
    "// To make some of the examples work we will also need RDD\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.graphx.util.GraphGenerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mcom.fasterxml.jackson.core.`type`.TypeReference\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.fasterxml.jackson.module.scala.DefaultScalaModule\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mbreeze.linalg._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mbreeze.plot._\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.fasterxml.jackson.core.`type`.TypeReference\n",
    "import com.fasterxml.jackson.module.scala.DefaultScalaModule\n",
    "import breeze.linalg._\n",
    "import breeze.plot._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bbf5510bb79c40b18ded72a586e03f7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mappName\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Spark_Graph_Algorithms_1\"\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val appName = \"Spark_Graph_Algorithms_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Setup the Logger\n",
    "\n",
    "To control the volume of log messages, change the log4j configuraiton programatically like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9546af49dfa44cebaca97724c6add0b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "//Logger.getLogger(\"org\").setLevel(Level.INFO)\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mlogger\u001b[39m: \u001b[32mLogger\u001b[39m = org.apache.log4j.Logger@194e1f5c"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "//Logger.getLogger(\"org\").setLevel(Level.INFO)\n",
    "\n",
    "val logger: Logger = Logger.getLogger(appName)\n",
    "Logger.getLogger(\"org.apache.spark\").setLevel(Level.WARN)\n",
    "Logger.getLogger(\"org.apache.spark.storage.BlockManager\").setLevel(Level.ERROR)\n",
    "logger.setLevel(Level.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Starting a Spark session\n",
    "\n",
    "### 2.1 Initialise Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "// close the spark session and spark context before starting a new one, if re-executing the notebook.\n",
    "\n",
    "//spark.stop()\n",
    "//sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "23/06/02 17:40:02 WARN Utils: Your hostname, icy resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/06/02 17:40:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkConf\u001b[39m: \u001b[32mSparkConf\u001b[39m = org.apache.spark.SparkConf@3c40d2b4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sparkConf = new SparkConf()\n",
    "             .setAppName(appName)\n",
    "             .setMaster(\"local[*]\")\n",
    "             //.setMaster(\"spark://sparkmaster320:7077\")\n",
    "             .set(\"spark.driver.extraClassPath\", \"/mnt/shared/lib/db2jcc4.jar,/mnt/shared/lib/breeze-viz_2.12-1.2.jar\")\n",
    "             .set(\"spark.executor.extraClassPath\", \"/mnt/shared/lib/db2jcc4.jar,/mnt/shared/lib/breeze-viz_2.12-1.2.jar\")\n",
    "             .set(\"spark.default.parallelism\", \"6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1e211dd28ca04c588d837d12902c7af8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/02 17:40:02 INFO SparkContext: Running Spark version 3.4.0\n",
      "23/06/02 17:40:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/06/02 17:40:02 INFO ResourceUtils: ==============================================================\n",
      "23/06/02 17:40:02 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/06/02 17:40:02 INFO ResourceUtils: ==============================================================\n",
      "23/06/02 17:40:02 INFO SparkContext: Submitted application: Spark_Graph_Algorithms_1\n",
      "23/06/02 17:40:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/06/02 17:40:02 INFO ResourceProfile: Limiting resource is cpu\n",
      "23/06/02 17:40:02 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/06/02 17:40:02 INFO SecurityManager: Changing view acls to: notebooker\n",
      "23/06/02 17:40:02 INFO SecurityManager: Changing modify acls to: notebooker\n",
      "23/06/02 17:40:02 INFO SecurityManager: Changing view acls groups to: \n",
      "23/06/02 17:40:02 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/06/02 17:40:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: notebooker; groups with view permissions: EMPTY; users with modify permissions: notebooker; groups with modify permissions: EMPTY\n",
      "23/06/02 17:40:03 INFO Utils: Successfully started service 'sparkDriver' on port 41933.\n",
      "23/06/02 17:40:03 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/06/02 17:40:03 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/06/02 17:40:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "23/06/02 17:40:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "23/06/02 17:40:03 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/06/02 17:40:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8955d34f-dc73-491a-8820-ec58bc547d03\n",
      "23/06/02 17:40:03 INFO MemoryStore: MemoryStore started with capacity 409.2 MiB\n",
      "23/06/02 17:40:03 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "23/06/02 17:40:03 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "23/06/02 17:40:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "23/06/02 17:40:03 INFO Executor: Starting executor ID driver on host 10.0.2.15\n",
      "23/06/02 17:40:03 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/mnt/shared/lib/db2jcc4.jar,/mnt/shared/lib/breeze-viz_2.12-1.2.jar,file:/mnt/src/spark_projs/graphx-algorithms/examples/breeze-viz_2.12-1.2.jar'\n",
      "23/06/02 17:40:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39033.\n",
      "23/06/02 17:40:03 INFO NettyBlockTransferService: Server created on 10.0.2.15:39033\n",
      "23/06/02 17:40:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "23/06/02 17:40:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 39033, None)\n",
      "23/06/02 17:40:03 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:39033 with 409.2 MiB RAM, BlockManagerId(driver, 10.0.2.15, 39033, None)\n",
      "23/06/02 17:40:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 39033, None)\n",
      "23/06/02 17:40:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 39033, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@3f719f23"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Apply the config to start a spark session:\n",
    "val spark = org.apache.spark.sql.SparkSession.builder()\n",
    "    .config(sparkConf)\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// For implicit conversions like converting RDDs to DataFrames\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set logging preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logger.getLogger(\"org.apache.spark.storage.BlockManager\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.storage.BlockManagerMaster\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.storage.BlockManagerMasterEndpoint\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.storage.BlockManagerInfo\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.storage.DiskBlockManager\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.storage.memory.MemoryStore\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.storage.ShuffleBlockFetcherIterator\").setLevel(Level.ERROR)\n",
    "\n",
    "Logger.getLogger(\"org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.scheduler.DAGScheduler\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.scheduler.TaskSchedulerImpl\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.scheduler.TaskSetManager\").setLevel(Level.ERROR)\n",
    "\n",
    "Logger.getLogger(\"org.apache.spark.SparkContext\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.executor.Executor\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.ui.JettyUtils\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.network.netty.NettyBlockTransferService\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.SparkEnv\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.util.Utils\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.rdd.HadoopRDD\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.MapOutputTrackerMasterEndpoint\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.hadoop.mapred.FileOutputCommitter\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.mapred.SparkHadoopMapRedUtil\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.internal.io.HadoopMapRedCommitProtocol\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.internal.io.SparkHadoopWriter\").setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "42ddce31bb4249bc89cc454a78399602"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msc\u001b[39m: \u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@668e7802"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Get information on Spark Session\n",
    "\n",
    "Use spark context and config objects to get essential information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1ac76653ab504ac3972b3e7df13eff68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Master: local[*], User: notebooker, Version: 3.4.0, Deployment mode: client\n",
      "Default Partitions: 2, Scheduling Mode: FIFO\n",
      "Configuration Parameter: spark.driver.host=10.0.2.15\n",
      "Configuration Parameter: spark.default.parallelism=6\n",
      "Configuration Parameter: spark.executor.extraClassPath=/mnt/shared/lib/db2jcc4.jar,/mnt/shared/lib/breeze-viz_2.12-1.2.jar\n",
      "Configuration Parameter: spark.app.name=Spark_Graph_Algorithms_1\n",
      "Configuration Parameter: spark.app.startTime=1685727602427\n",
      "Configuration Parameter: spark.master=local[*]\n",
      "Configuration Parameter: spark.app.id=local-1685727603602\n",
      "Configuration Parameter: spark.executor.id=driver\n",
      "Configuration Parameter: spark.driver.extraClassPath=/mnt/shared/lib/db2jcc4.jar,/mnt/shared/lib/breeze-viz_2.12-1.2.jar\n",
      "Configuration Parameter: spark.driver.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "Configuration Parameter: spark.driver.port=41933\n",
      "Configuration Parameter: spark.executor.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mconfig\u001b[39m: \u001b[32mSparkConf\u001b[39m = org.apache.spark.SparkConf@7db70d1d"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Spark Master: %s, User: %s, Version: %s, Deployment mode: %s\".format(\n",
    "        sc.master, sc.sparkUser, sc.version, sc.deployMode\n",
    "    ))\n",
    "\n",
    "println(\"Default Partitions: %d, Scheduling Mode: %s\".format(\n",
    "         sc.defaultMinPartitions, sc.getSchedulingMode\n",
    "    ))\n",
    "\n",
    "val config = sc.getConf\n",
    "\n",
    "for ((k,v) <- config.getAll) println(s\"Configuration Parameter: $k=$v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load sample graph data\n",
    "\n",
    "Data can be loaded into a graph by reading from an edgelist file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mprintEdges\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define convenience function to print all edges of a graph with vertex data:\n",
    "def printEdges[V, E]( graph: Graph[V, E] ): Unit = {\n",
    "    \n",
    "    val facts: RDD[String] = graph.triplets.map(triplet => \n",
    "      \" \" + triplet.toTuple._1 + \" --[\" + triplet.toTuple._3 + \"]--> \" + triplet.toTuple._2 );\n",
    "\n",
    "    facts.collect.foreach(println(_))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mprintVertices\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def printVertices[V, E](graph:Graph[_, _]):Unit = {\n",
    "    graph.vertices.map(\n",
    "      vd => \"Vertex ID = \" + vd._1 + \": \" + vd._2\n",
    "    ).collect.foreach(println(_))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/02 17:40:06 INFO FileInputFormat: Total input files to process : 1\n",
      "23/06/02 17:40:07 INFO GraphLoader: It took 1423 ms to load the edges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (10,[0,0.0]) --[1.0]--> (20,[0,0.0])\n",
      " (20,[0,0.0]) --[1.0]--> (30,[0,0.0])\n",
      " (30,[0,0.0]) --[1.0]--> (10,[0,0.0])\n",
      " (70,[0,0.0]) --[1.0]--> (80,[0,0.0])\n",
      " (40,[0,0.0]) --[1.0]--> (50,[0,0.0])\n",
      " (50,[0,0.0]) --[1.0]--> (60,[0,0.0])\n",
      " (60,[0,0.0]) --[1.0]--> (20,[0,0.0])\n",
      " (80,[0,0.0]) --[1.0]--> (90,[0,0.0])\n",
      " (90,[0,0.0]) --[1.0]--> (70,[0,0.0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph1\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mRow\u001b[39m, \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@28da41f9"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// read from edgelist file\n",
    "val graph1 = GraphLoader\n",
    "      .edgeListFile(sc,\n",
    "                    \"../src/test/resources/graph1_edgelist.txt\",\n",
    "                    edgeStorageLevel=StorageLevel.MEMORY_AND_DISK,\n",
    "                    vertexStorageLevel=StorageLevel.MEMORY_AND_DISK)\n",
    "      .mapEdges(e => e.attr.toDouble)\n",
    "      // here, we define vertex with a Row that holds a Long data type and a Double data type\n",
    "      // these would be sufficient to hold results of most graph algorithms\n",
    "      .mapVertices[Row]((vid, data) => Row(0L, 0.0));\n",
    "\n",
    "// print out the graph:\n",
    "printEdges( graph1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating graphs from vertex and edge data\n",
    "Data can also be fed in via RDDs of edges and vertices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36musers\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mVertexId\u001b[39m, (\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m))] = ParallelCollectionRDD[23] at makeRDD at cmd15.sc:2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create an RDD for the vertices\n",
    "val users: RDD[(VertexId, (String, String))] =\n",
    "  sc.makeRDD( Array(\n",
    "        (3L, (\"rxin\", \"student\"))\n",
    "      , (7L, (\"jgonzal\", \"postdoc\"))\n",
    "      , (1L, (\"somebody\", \"postdoc\"))\n",
    "      , (5L, (\"franklin\", \"prof\"))\n",
    "      , (2L, (\"istoica\", \"prof\"))\n",
    "      , (10L, (\"hoityToity\", \"student\"))\n",
    "     )\n",
    "   ).persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrelationships\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mEdge\u001b[39m[\u001b[32mRow\u001b[39m]] = ParallelCollectionRDD[24] at makeRDD at cmd16.sc:2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create an RDD for edges\n",
    "val relationships: RDD[Edge[Row]] =\n",
    "  sc.makeRDD(\n",
    "      Array(\n",
    "        Edge(3L, 7L,  /*\"collab\"   */ Row(\"collab\", 0.0, 0L))\n",
    "      , Edge(5L, 3L,  /*\"advisor\"  */ Row(\"advisor\", 0.0, 0L))\n",
    "      , Edge(2L, 5L,  /*\"colleague\"*/ Row(\"colleague\", 0.0, 0L))\n",
    "      , Edge(5L, 7L,  /*\"advisor\"  */ Row(\"advisor\", 0.0, 0L))\n",
    "      , Edge(10L, 5L, /*\"friend\"   */ Row(\"friend\", 0.0, 0L))\n",
    "      , Edge(10L, 1L, /*\"friend\"   */ Row(\"friend\", 0.0, 0L))\n",
    "      )\n",
    "    ).persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdefaultUser\u001b[39m: (\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m) = (\u001b[32m\"Jane\"\u001b[39m, \u001b[32m\"Missing\"\u001b[39m)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Define a default user in case there are relationship with missing user\n",
    "val defaultUser = (\"Jane\", \"Missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph2\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m), \u001b[32mRow\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@66bed1c7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Build the initial Graph\n",
    "val graph2 = Graph(users, relationships, defaultUser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraphGrid\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mInt\u001b[39m), \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@71b1c65d\n",
       "\u001b[36mgraphStar\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mInt\u001b[39m, \u001b[32mInt\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@12d04e39\n",
       "\u001b[36mgraphLogNormGen\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mLong\u001b[39m, \u001b[32mInt\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@2a587bbd"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graphGrid = GraphGenerators.gridGraph(sc, 4, 4)\n",
    "val graphStar = GraphGenerators.starGraph(sc, 8)\n",
    "val graphLogNormGen = GraphGenerators.logNormalGraph(sc, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## View Basic GraphX Properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mprintGraphProperties\u001b[39m"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def printGraphProperties( graph: Graph[_,_] ): Unit = {\n",
    "    // graph operators:\n",
    "    println( \"Num of edges = \" + graph.numEdges )\n",
    "    println( \"Num of vertices = \" + graph.numVertices )\n",
    "    println( \"Num of inDegrees = \" + graph.inDegrees.count() )\n",
    "    println( \"Num of outDegrees = \" + graph.outDegrees.count() )\n",
    "    println( \"Num of degrees = \" + graph.degrees.count() )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mprintNeighbors\u001b[39m"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def printNeighbors[V, D, E](graph: Graph[_, _], edgeDirection: EdgeDirection):Unit = {\n",
    "    graph.collectNeighborIds(edgeDirection).collect.foreach(\n",
    "      x =>\n",
    "        println(\"Neighbors of \" + x._1 + \" (\"+ edgeDirection +\") are: \" + x._2.mkString(\",\") )\n",
    "    );\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex ID = 1: (somebody,postdoc)\n",
      "Vertex ID = 7: (jgonzal,postdoc)\n",
      "Vertex ID = 2: (istoica,prof)\n",
      "Vertex ID = 3: (rxin,student)\n",
      "Vertex ID = 10: (hoityToity,student)\n",
      "Vertex ID = 5: (franklin,prof)\n"
     ]
    }
   ],
   "source": [
    "printVertices( graph2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3,(rxin,student)) --[[collab,0.0,0]]--> (7,(jgonzal,postdoc))\n",
      " (5,(franklin,prof)) --[[advisor,0.0,0]]--> (3,(rxin,student))\n",
      " (2,(istoica,prof)) --[[colleague,0.0,0]]--> (5,(franklin,prof))\n",
      " (5,(franklin,prof)) --[[advisor,0.0,0]]--> (7,(jgonzal,postdoc))\n",
      " (10,(hoityToity,student)) --[[friend,0.0,0]]--> (5,(franklin,prof))\n",
      " (10,(hoityToity,student)) --[[friend,0.0,0]]--> (1,(somebody,postdoc))\n"
     ]
    }
   ],
   "source": [
    "// print out the graph:\n",
    "printEdges( graph2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of edges = 9\n",
      "Num of vertices = 9\n",
      "Num of inDegrees = 8\n",
      "Num of outDegrees = 9\n",
      "Num of degrees = 9\n"
     ]
    }
   ],
   "source": [
    "// print out basic properties:\n",
    "printGraphProperties(graph1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of edges = 6\n",
      "Num of vertices = 6\n",
      "Num of inDegrees = 4\n",
      "Num of outDegrees = 4\n",
      "Num of degrees = 6\n"
     ]
    }
   ],
   "source": [
    "// print out basic properties:\n",
    "printGraphProperties(graph2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Projections / Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 1: Count all the edges where src > dst\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "println( \"Graph 1: Count all the edges where src > dst\")\n",
    "println( graph1.edges.filter(e => e.srcId > e.dstId).count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph LogNormGen: Count all the edges where src > dst\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "println( \"Graph LogNormGen: Count all the edges where src > dst\")\n",
    "println( graphLogNormGen.edges.filter(e => e.srcId > e.dstId).count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 1: Reversed edge directions\n",
      " (10,[0,0.0]) --[1.0]--> (30,[0,0.0])\n",
      " (20,[0,0.0]) --[1.0]--> (10,[0,0.0])\n",
      " (30,[0,0.0]) --[1.0]--> (20,[0,0.0])\n",
      " (80,[0,0.0]) --[1.0]--> (70,[0,0.0])\n",
      " (20,[0,0.0]) --[1.0]--> (60,[0,0.0])\n",
      " (50,[0,0.0]) --[1.0]--> (40,[0,0.0])\n",
      " (60,[0,0.0]) --[1.0]--> (50,[0,0.0])\n",
      " (70,[0,0.0]) --[1.0]--> (90,[0,0.0])\n",
      " (90,[0,0.0]) --[1.0]--> (80,[0,0.0])\n"
     ]
    }
   ],
   "source": [
    "println( \"Graph 1: Reversed edge directions\")\n",
    "printEdges( graph1.reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3,(rxin,student)) --[[collab,0.0,0]]--> (7,(jgonzal,postdoc))\n",
      " (5,(franklin,prof)) --[[advisor,0.0,0]]--> (3,(rxin,student))\n",
      " (2,(istoica,prof)) --[[colleague,0.0,0]]--> (5,(franklin,prof))\n",
      " (5,(franklin,prof)) --[[advisor,0.0,0]]--> (7,(jgonzal,postdoc))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph2_nofriends\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m), \u001b[32mRow\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@bc62b1f"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph2_nofriends = graph2.subgraph(epred = edgetriplet => edgetriplet.attr(0) != \"friend\")\n",
    "printEdges(graph2_nofriends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (10,(hoityToity,student)) --[[friend,0.0,0]]--> (5,(franklin,prof))\n",
      " (10,(hoityToity,student)) --[[friend,0.0,0]]--> (1,(somebody,postdoc))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph2_friends\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m), \u001b[32mRow\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@6fe343f1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph2_friends = graph2.subgraph(epred = edgetriplet => edgetriplet.attr(0) == \"friend\")\n",
    "printEdges(graph2_friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Graph 1, a subgraph where src vertex id > 10\n",
      " (20,[0,0.0]) --[1.0]--> (30,[0,0.0])\n",
      " (70,[0,0.0]) --[1.0]--> (80,[0,0.0])\n",
      " (40,[0,0.0]) --[1.0]--> (50,[0,0.0])\n",
      " (50,[0,0.0]) --[1.0]--> (60,[0,0.0])\n",
      " (60,[0,0.0]) --[1.0]--> (20,[0,0.0])\n",
      " (80,[0,0.0]) --[1.0]--> (90,[0,0.0])\n",
      " (90,[0,0.0]) --[1.0]--> (70,[0,0.0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph3\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mRow\u001b[39m, \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@6a44f5b1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph3 = graph1.subgraph(vpred = (id, attr) => id > 10)\n",
    "println( \"From Graph 1, a subgraph where src vertex id > 10\")\n",
    "printEdges(graph3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (20,[0,0.0]) --[1.0]--> (30,[0,0.0])\n",
      " (70,[0,0.0]) --[1.0]--> (80,[0,0.0])\n",
      " (40,[0,0.0]) --[1.0]--> (50,[0,0.0])\n",
      " (50,[0,0.0]) --[1.0]--> (60,[0,0.0])\n",
      " (60,[0,0.0]) --[1.0]--> (20,[0,0.0])\n",
      " (80,[0,0.0]) --[1.0]--> (90,[0,0.0])\n",
      " (90,[0,0.0]) --[1.0]--> (70,[0,0.0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph5\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mRow\u001b[39m, \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@4051f81c"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph5 = graph1.mask(graph3)\n",
    "printEdges(graph5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3,(rxin,student)) --[[collab,0.0,0]]--> (7,(jgonzal,postdoc))\n",
      " (5,(franklin,prof)) --[[advisor,0.0,0]]--> (3,(rxin,student))\n",
      " (2,(istoica,prof)) --[[colleague,0.0,0]]--> (5,(franklin,prof))\n",
      " (5,(franklin,prof)) --[[advisor,0.0,0]]--> (7,(jgonzal,postdoc))\n",
      " (10,(hoityToity,student)) --[[friend,0.0,0]]--> (5,(franklin,prof))\n",
      " (10,(hoityToity,student)) --[[friend,0.0,0]]--> (1,(somebody,postdoc))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph6\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m), \u001b[32mRow\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@2e055148"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph6 = graph2.groupEdges( (x, y) => Row(\"friend\"))\n",
    "printEdges(graph6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbors of 1 (EdgeDirection.Out) are: \n",
      "Neighbors of 7 (EdgeDirection.Out) are: \n",
      "Neighbors of 2 (EdgeDirection.Out) are: 5\n",
      "Neighbors of 3 (EdgeDirection.Out) are: 7\n",
      "Neighbors of 10 (EdgeDirection.Out) are: 5,1\n",
      "Neighbors of 5 (EdgeDirection.Out) are: 3,7\n"
     ]
    }
   ],
   "source": [
    "printNeighbors(graph6, EdgeDirection.Out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbors of 1 (EdgeDirection.In) are: 10\n",
      "Neighbors of 7 (EdgeDirection.In) are: 3,5\n",
      "Neighbors of 2 (EdgeDirection.In) are: \n",
      "Neighbors of 3 (EdgeDirection.In) are: 5\n",
      "Neighbors of 10 (EdgeDirection.In) are: \n",
      "Neighbors of 5 (EdgeDirection.In) are: 2,10\n"
     ]
    }
   ],
   "source": [
    "printNeighbors(graph2, EdgeDirection.In)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Vertex data of a graph\n",
    "\n",
    "Load external data as properties of the vertices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (10,[0,0.0]) --[1.0]--> (20,[0,0.0])\n",
      " (20,[0,0.0]) --[1.0]--> (30,[0,0.0])\n",
      " (30,[0,0.0]) --[1.0]--> (10,[0,0.0])\n",
      " (70,[0,0.0]) --[1.0]--> (80,[0,0.0])\n",
      " (40,[0,0.0]) --[1.0]--> (50,[0,0.0])\n",
      " (50,[0,0.0]) --[1.0]--> (60,[0,0.0])\n",
      " (60,[0,0.0]) --[1.0]--> (20,[0,0.0])\n",
      " (80,[0,0.0]) --[1.0]--> (90,[0,0.0])\n",
      " (90,[0,0.0]) --[1.0]--> (70,[0,0.0])\n"
     ]
    }
   ],
   "source": [
    "printEdges(graph1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlabelsRDD\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mDouble\u001b[39m)] = ParallelCollectionRDD[208] at makeRDD at cmd46.sc:1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelsRDD = sc.makeRDD(\n",
    "    Seq(\n",
    "        (10L,100.0),\n",
    "        (20L,200.0),\n",
    "        (30L,300.0),\n",
    "        (70L,700.0),\n",
    "        (40L,400.0),\n",
    "        (50L,500.0),\n",
    "        (60L,600.0),\n",
    "        (80L,800.0),\n",
    "        (90L,900.0),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (10,[0,100.0]) --[1.0]--> (20,[0,200.0])\n",
      " (20,[0,200.0]) --[1.0]--> (30,[0,300.0])\n",
      " (30,[0,300.0]) --[1.0]--> (10,[0,100.0])\n",
      " (70,[0,700.0]) --[1.0]--> (80,[0,800.0])\n",
      " (40,[0,400.0]) --[1.0]--> (50,[0,500.0])\n",
      " (50,[0,500.0]) --[1.0]--> (60,[0,600.0])\n",
      " (60,[0,600.0]) --[1.0]--> (20,[0,200.0])\n",
      " (80,[0,800.0]) --[1.0]--> (90,[0,900.0])\n",
      " (90,[0,900.0]) --[1.0]--> (70,[0,700.0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph7\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mRow\u001b[39m, \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@7ae9bf2c"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Join RDDs with the graph\n",
    "val graph7 = graph1.joinVertices(labelsRDD:RDD[Tuple2[Long, Double]])( (vid:VertexId, vd:Row, U:Double) => Row(vd(0), U))\n",
    "\n",
    "printEdges(graph7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val graph8 = graph1.outerJoinVertices[U, VD2](other: RDD[(VertexId, U)])\n",
    "      (mapFunc: (VertexId, VD, Option[U]) => VD2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting vertex data from graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph7vertices\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mVertexId\u001b[39m, \u001b[32mAny\u001b[39m, \u001b[32mAny\u001b[39m)] = MapPartitionsRDD[229] at map at cmd60.sc:1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph7vertices = graph7.vertices.map( x => (x._1, x._2(0), x._2(1)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres61\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mVertexId\u001b[39m, \u001b[32mAny\u001b[39m, \u001b[32mAny\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m80L\u001b[39m, \u001b[32m0L\u001b[39m, \u001b[32m800.0\u001b[39m),\n",
       "  (\u001b[32m30L\u001b[39m, \u001b[32m0L\u001b[39m, \u001b[32m300.0\u001b[39m),\n",
       "  (\u001b[32m50L\u001b[39m, \u001b[32m0L\u001b[39m, \u001b[32m500.0\u001b[39m),\n",
       "  (\u001b[32m40L\u001b[39m, \u001b[32m0L\u001b[39m, \u001b[32m400.0\u001b[39m),\n",
       "  (\u001b[32m90L\u001b[39m, \u001b[32m0L\u001b[39m, \u001b[32m900.0\u001b[39m),\n",
       "  (\u001b[32m70L\u001b[39m, \u001b[32m0L\u001b[39m, \u001b[32m700.0\u001b[39m),\n",
       "  (\u001b[32m20L\u001b[39m, \u001b[32m0L\u001b[39m, \u001b[32m200.0\u001b[39m),\n",
       "  (\u001b[32m60L\u001b[39m, \u001b[32m0L\u001b[39m, \u001b[32m600.0\u001b[39m),\n",
       "  (\u001b[32m10L\u001b[39m, \u001b[32m0L\u001b[39m, \u001b[32m100.0\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph7vertices.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "//val graph7_df = spark.createDataFrame(graph7.vertices.map( x => (x._1, x._2(0), x._2(1)) )).toDF(Seq(\"Vertex_id\", \"col2\", \"col3\"):_*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Saving graphs as object files\n",
    "All data and can be saved to disk.\n",
    "\n",
    "The default data format for saving to disk is Parquet which also compresses the data structure using SNAPPY compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/02 18:20:50 INFO SequenceFileRDDFunctions: Saving as sequence file of type (NullWritable,BytesWritable)\n",
      "23/06/02 18:20:50 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
      "23/06/02 18:20:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/02 18:20:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/02 18:20:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/02 18:20:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/02 18:20:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/02 18:20:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/02 18:20:51 INFO FileOutputCommitter: Saved output of task 'attempt_20230602182050693452137974274580_0246_m_000001_0' to file:/mnt/src/spark_projs/graphx-algorithms/examples/graph1_vertices.obj/_temporary/0/task_20230602182050693452137974274580_0246_m_000001\n",
      "23/06/02 18:20:51 INFO FileOutputCommitter: Saved output of task 'attempt_20230602182050693452137974274580_0246_m_000000_0' to file:/mnt/src/spark_projs/graphx-algorithms/examples/graph1_vertices.obj/_temporary/0/task_20230602182050693452137974274580_0246_m_000000\n"
     ]
    }
   ],
   "source": [
    "// Save vertices RDD to disk\n",
    "graph1.vertices.saveAsObjectFile(\"graph1_vertices.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/02 18:20:54 INFO SequenceFileRDDFunctions: Saving as sequence file of type (NullWritable,BytesWritable)\n",
      "23/06/02 18:20:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/02 18:20:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/02 18:20:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/02 18:20:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/02 18:20:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/02 18:20:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/02 18:20:54 INFO FileOutputCommitter: Saved output of task 'attempt_202306021820544756191222913200626_0248_m_000000_0' to file:/mnt/src/spark_projs/graphx-algorithms/examples/graph1_edges.obj/_temporary/0/task_202306021820544756191222913200626_0248_m_000000\n",
      "23/06/02 18:20:54 INFO FileOutputCommitter: Saved output of task 'attempt_202306021820544756191222913200626_0248_m_000001_0' to file:/mnt/src/spark_projs/graphx-algorithms/examples/graph1_edges.obj/_temporary/0/task_202306021820544756191222913200626_0248_m_000001\n"
     ]
    }
   ],
   "source": [
    "// then, save edges RDD to disk\n",
    "graph1.edges.saveAsObjectFile(\"graph1_edges.obj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving graph to JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/02 18:20:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/02 18:20:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/02 18:20:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/02 18:20:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/02 18:20:58 INFO FileOutputCommitter: Saved output of task 'attempt_202306021820582503446850220815044_0254_m_000000_0' to file:/mnt/src/spark_projs/graphx-algorithms/examples/graph2_vertices_json/_temporary/0/task_202306021820582503446850220815044_0254_m_000000\n"
     ]
    }
   ],
   "source": [
    "graph2.vertices.map(x => {\n",
    "    val mapper = new com.fasterxml.jackson.databind.ObjectMapper()\n",
    "    mapper.registerModule(\n",
    "    com.fasterxml.jackson.module.scala.DefaultScalaModule)\n",
    "    val writer = new java.io.StringWriter()\n",
    "    mapper.writeValue(writer, x)\n",
    "    writer.toString\n",
    "    }).coalesce(1,true).saveAsTextFile(\"graph2_vertices_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/02 18:21:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/02 18:21:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/02 18:21:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/02 18:21:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/02 18:21:01 INFO FileOutputCommitter: Saved output of task 'attempt_202306021821004346008190923540698_0260_m_000000_0' to file:/mnt/src/spark_projs/graphx-algorithms/examples/graph2_edges_json/_temporary/0/task_202306021821004346008190923540698_0260_m_000000\n"
     ]
    }
   ],
   "source": [
    "graph2.edges.mapPartitions(edges => {\n",
    "val mapper = new com.fasterxml.jackson.databind.ObjectMapper();\n",
    "mapper.registerModule(DefaultScalaModule)\n",
    "val writer = new java.io.StringWriter()\n",
    "edges.map(e => {writer.getBuffer.setLength(0)\n",
    "mapper.writeValue(writer, e)\n",
    "writer.toString})\n",
    "}).coalesce(1,true).saveAsTextFile(\"graph2_edges_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/02 18:21:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/02 18:21:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/02 18:21:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/02 18:21:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/02 18:21:08 INFO FileOutputCommitter: Saved output of task 'attempt_202306021821085844259637603774879_0266_m_000000_0' to file:/mnt/src/spark_projs/graphx-algorithms/examples/graph3_vertices_json/_temporary/0/task_202306021821085844259637603774879_0266_m_000000\n"
     ]
    }
   ],
   "source": [
    "graph3.vertices.mapPartitions(vertices => {\n",
    "val mapper = new com.fasterxml.jackson.databind.ObjectMapper()\n",
    "mapper.registerModule(DefaultScalaModule)\n",
    "val writer = new java.io.StringWriter()\n",
    "vertices.map(v => {writer.getBuffer.setLength(0)\n",
    "mapper.writeValue(writer, v)\n",
    "writer.toString})\n",
    "}).coalesce(1,true).saveAsTextFile(\"graph3_vertices_json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving graph to GEXF (Gephi) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcollectAsGexf\u001b[39m"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collectAsGexf[VD,ED](g:Graph[VD,ED]):String = {\n",
    "    \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\" +\n",
    "    \"<gexf xmlns=\\\"http://www.gexf.net/1.2draft\\\" version=\\\"1.2\\\">\\n\" +\n",
    "    \" <graph mode=\\\"static\\\" defaultedgetype=\\\"directed\\\">\\n\" +\n",
    "    \" <nodes>\\n\" +\n",
    "    g.vertices.map(v => \" <node id=\\\"\" + v._1 + \"\\\" label=\\\"\" +\n",
    "    v._2 + \"\\\" />\\n\").collect.mkString +\n",
    "    \" </nodes>\\n\" +\n",
    "    \" <edges>\\n\" +\n",
    "    g.edges.map(e => \" <edge source=\\\"\" + e.srcId +\n",
    "    \"\\\" target=\\\"\" + e.dstId + \"\\\" label=\\\"\" + e.attr +\n",
    "    \"\\\" />\\n\").collect.mkString +\n",
    "    \" </edges>\\n\" +\n",
    "    \" </graph>\\n\" +\n",
    "    \"</gexf>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpw\u001b[39m: \u001b[32mjava\u001b[39m.\u001b[32mio\u001b[39m.\u001b[32mPrintWriter\u001b[39m = java.io.PrintWriter@f928bc1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pw = new java.io.PrintWriter(\"graph2.gexf\")\n",
    "pw.write(collectAsGexf(graph2))\n",
    "pw.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Stop the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/02 18:22:12 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
