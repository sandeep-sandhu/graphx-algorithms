{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Algorithms - Part 2 - Using provided GrophX Algorithms\n",
    "\n",
    "Using algorithms already implemented in the Apache Spark GraphX platform\n",
    "\n",
    "The following topics have been covered:\n",
    "\n",
    "  1. Initialising Spark Environment\n",
    "  1. Loading and constructing example graphs\n",
    "  1. Using out-of-the-box algorithms\n",
    "  1. Using the send message method to write a simple algorithm\n",
    "  1. Use the Pregel API to write a simple Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Initialising the Apache Spark environment\n",
    "\n",
    "When running in a jupyter notebook, sometimes the required libraries may not exist in the classpath.\n",
    "\n",
    "Load all the essential apache spark libraries from maven public repositories at runtime in this manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "a7ca08d823f240129c8c3b8e1deb4fa2",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                          \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                        \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-core:3.4.0`\n",
    "import $ivy.`org.apache.spark::spark-mllib-local:3.4.0`\n",
    "import $ivy.`org.apache.spark::spark-mllib:3.4.0`\n",
    "import $ivy.`org.apache.spark::spark-graphx:3.4.0`\n",
    "import $ivy.`org.apache.spark::spark-streaming:3.4.0`\n",
    "import $ivy.`org.apache.spark::spark-tags:3.4.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.scalanlp::breeze-viz:1.2`\n",
    "import $ivy.`org.jfree:jfreechart:1.5.4`\n",
    "import $ivy.`org.creativescala::doodle-core:0.9.21`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                   \u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`com.fasterxml.jackson.core:jackson-databind:2.15.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.2 Import the Spark Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "732e8d55868d4b848e8a1aa50e5a9a12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39m"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "cc8495eee18e4c97abe4af3ac3e87209"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.linalg.{Matrix, Vectors}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Row\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Dataset\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.{col, udf, _}\u001b[39m"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.{Matrix, Vectors}\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.functions.{col, udf, _}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "ec63a53bf3c4458984c0ab716e5aa0b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.graphx._\n",
       "// To make some of the examples work we will also need RDD\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.rdd.RDD\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.storage.StorageLevel\u001b[39m"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.graphx._\n",
    "// To make some of the examples work we will also need RDD\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.storage.StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mcom.fasterxml.jackson.core.`type`.TypeReference\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.fasterxml.jackson.module.scala.DefaultScalaModule\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mbreeze.linalg._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mbreeze.plot._\u001b[39m"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.fasterxml.jackson.core.`type`.TypeReference\n",
    "import com.fasterxml.jackson.module.scala.DefaultScalaModule\n",
    "import breeze.linalg._\n",
    "import breeze.plot._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "bbf5510bb79c40b18ded72a586e03f7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mappName\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Spark_Graph_Algorithms2\"\u001b[39m"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val appName = \"Spark_Graph_Algorithms2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Setup the Logger\n",
    "\n",
    "To control the volume of log messages, change the log4j configuraiton programatically like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "9546af49dfa44cebaca97724c6add0b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "//Logger.getLogger(\"org\").setLevel(Level.INFO)\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mlogger\u001b[39m: \u001b[32mLogger\u001b[39m = org.apache.log4j.Logger@f4880fc"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "//Logger.getLogger(\"org\").setLevel(Level.INFO)\n",
    "\n",
    "val logger: Logger = Logger.getLogger(appName)\n",
    "Logger.getLogger(\"org.apache.spark\").setLevel(Level.WARN)\n",
    "logger.setLevel(Level.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Create Spark session\n",
    "\n",
    "### 2.1 Initialise Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkConf\u001b[39m: \u001b[32mSparkConf\u001b[39m = org.apache.spark.SparkConf@69a5de46"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sparkConf = new SparkConf()\n",
    "             .setAppName(appName)\n",
    "             .setMaster(\"local[*]\")\n",
    "             //.setMaster(\"spark://sparkmaster320:7077\")\n",
    "             .set(\"spark.driver.extraClassPath\", \"/mnt/shared/lib/db2jcc4.jar,/mnt/shared/lib/breeze-viz_2.12-1.2.jar\")\n",
    "             .set(\"spark.executor.extraClassPath\", \"/mnt/shared/lib/db2jcc4.jar,/mnt/shared/lib/breeze-viz_2.12-1.2.jar\")\n",
    "             .set(\"spark.default.parallelism\", \"6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "1e211dd28ca04c588d837d12902c7af8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/02 19:03:02 INFO ResourceUtils: ==============================================================\n",
      "23/06/02 19:03:02 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/06/02 19:03:02 INFO ResourceUtils: ==============================================================\n",
      "23/06/02 19:03:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/06/02 19:03:02 INFO ResourceProfile: Limiting resource is cpu\n",
      "23/06/02 19:03:02 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/06/02 19:03:02 INFO SecurityManager: Changing view acls to: notebooker\n",
      "23/06/02 19:03:02 INFO SecurityManager: Changing modify acls to: notebooker\n",
      "23/06/02 19:03:02 INFO SecurityManager: Changing view acls groups to: \n",
      "23/06/02 19:03:02 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/06/02 19:03:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: notebooker; groups with view permissions: EMPTY; users with modify permissions: notebooker; groups with modify permissions: EMPTY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@3006e12f"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Apply the config to start a spark session:\n",
    "val spark = org.apache.spark.sql.SparkSession.builder()\n",
    "    .config(sparkConf)\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "9546af49dfa44cebaca97724c6add0b1"
   },
   "outputs": [],
   "source": [
    "Logger.getLogger(\"org.apache.spark.storage.BlockManager\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.storage.BlockManagerMaster\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.storage.BlockManagerMasterEndpoint\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.storage.BlockManagerInfo\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.storage.DiskBlockManager\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.storage.memory.MemoryStore\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.storage.ShuffleBlockFetcherIterator\").setLevel(Level.ERROR)\n",
    "\n",
    "Logger.getLogger(\"org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.scheduler.DAGScheduler\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.scheduler.TaskSchedulerImpl\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.scheduler.TaskSetManager\").setLevel(Level.ERROR)\n",
    "\n",
    "Logger.getLogger(\"org.apache.spark.SparkContext\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.executor.Executor\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.ui.JettyUtils\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.network.netty.NettyBlockTransferService\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.SparkEnv\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.util.Utils\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.rdd.HadoopRDD\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.MapOutputTrackerMasterEndpoint\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.hadoop.mapred.FileOutputCommitter\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.mapred.SparkHadoopMapRedUtil\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.internal.io.HadoopMapRedCommitProtocol\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.internal.io.SparkHadoopWriter\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.rdd.ZippedPartitionsRDD2\").setLevel(Level.ERROR)\n",
    "Logger.getLogger(\"org.apache.spark.rdd.MapPartitionsRDD\").setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "42ddce31bb4249bc89cc454a78399602"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msc\u001b[39m: \u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@2d1e28bd"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// For implicit conversions like converting RDDs to DataFrames\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load sample graph data\n",
    "\n",
    "Data can be loaded into a graph by reading from an edgelist file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/02 19:03:05 INFO FileInputFormat: Total input files to process : 1\n",
      "23/06/02 19:03:05 INFO GraphLoader: It took 109 ms to load the edges\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph1\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mDouble\u001b[39m), \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@6cb3084e"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// read from edgelist file\n",
    "val graph1 = GraphLoader\n",
    "      .edgeListFile(sc,\n",
    "                    \"../src/test/resources/graph1_edgelist.txt\",\n",
    "                    edgeStorageLevel=StorageLevel.MEMORY_AND_DISK,\n",
    "                    vertexStorageLevel=StorageLevel.MEMORY_AND_DISK)\n",
    "      .mapEdges(e => e.attr.toDouble)\n",
    "      .mapVertices[(Long, Double)]((vid, data) => (vid.toLong, 0.0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can also be fed in via RDDs of edges and vertices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36musers\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mVertexId\u001b[39m, (\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m))] = ParallelCollectionRDD[17] at makeRDD at cmd110.sc:2"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create an RDD for the vertices\n",
    "val users: RDD[(VertexId, (String, String))] =\n",
    "  sc.makeRDD( Array(\n",
    "        (3L, (\"rxin\", \"student\"))\n",
    "      , (7L, (\"jgonzal\", \"postdoc\"))\n",
    "      , (1L, (\"somebody\", \"postdoc\"))\n",
    "      , (5L, (\"franklin\", \"prof\"))\n",
    "      , (2L, (\"istoica\", \"prof\"))\n",
    "      , (10L, (\"hoityToity\", \"student\"))\n",
    "     )\n",
    "   ).persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrelationships\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mEdge\u001b[39m[\u001b[32mString\u001b[39m]] = ParallelCollectionRDD[18] at makeRDD at cmd111.sc:2"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create an RDD for edges\n",
    "val relationships: RDD[Edge[String]] =\n",
    "  sc.makeRDD(\n",
    "      Array(\n",
    "      Edge(3L, 7L, \"collab\")\n",
    "      , Edge(5L, 3L, \"advisor\")\n",
    "      , Edge(2L, 5L, \"colleague\")\n",
    "      , Edge(5L, 7L, \"pi\")\n",
    "      , Edge(10L, 5L, \"friend\")\n",
    "      , Edge(10L, 1L, \"friend\")\n",
    "      )\n",
    "    ).persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdefaultUser\u001b[39m: (\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m) = (\u001b[32m\"John Doe\"\u001b[39m, \u001b[32m\"Missing\"\u001b[39m)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Define a default user in case there are relationship with missing user\n",
    "val defaultUser = (\"John Doe\", \"Missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph2\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m), \u001b[32mString\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@3a6afacb"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Build the initial Graph\n",
    "val graph2 = Graph(users, relationships, defaultUser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.graphx.util.GraphGenerators\u001b[39m"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx.util.GraphGenerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph3\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mInt\u001b[39m), \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@3acb48fe\n",
       "\u001b[36mgraph4\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mInt\u001b[39m, \u001b[32mInt\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@4420d662\n",
       "\u001b[36mgraph5\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mLong\u001b[39m, \u001b[32mInt\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@302eebf5"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph3 = GraphGenerators.gridGraph(sc, 4, 4)\n",
    "val graph4 = GraphGenerators.starGraph(sc, 8)\n",
    "val graph5 = GraphGenerators.logNormalGraph(sc, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Utility functions for viewing graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mprintEdges\u001b[39m"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define convenience function to print all edges of a graph:\n",
    "def printEdges[V, E]( graph: Graph[V, E] ): Unit = {\n",
    "    \n",
    "    val facts: RDD[String] = graph.triplets.map(triplet => \n",
    "      \" \" + triplet.toTuple._1 + \" --[\" + triplet.toTuple._3 + \"]--> \" + triplet.toTuple._2 );\n",
    "\n",
    "    facts.collect.foreach(println(_))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mprintGraphProperties\u001b[39m"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def printGraphProperties( graph: Graph[_,_] ): Unit = {\n",
    "    // graph operators:\n",
    "    println( \"Num of edges = \" + graph.numEdges )\n",
    "    println( \"Num of vertices = \" + graph.numVertices )\n",
    "    println( \"Num of inDegrees = \" + graph.inDegrees.count() )\n",
    "    println( \"Num of outDegrees = \" + graph.outDegrees.count() )\n",
    "    println( \"Num of degrees = \" + graph.degrees.count() )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mprintNeighbors\u001b[39m"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def printNeighbors[V, D, E](graph: Graph[_, _], edgeDirection: EdgeDirection):Unit = {\n",
    "    graph.collectNeighborIds(edgeDirection).collect.foreach(\n",
    "      x =>\n",
    "        println(\"Neighbors of \" + x._1 + \" (\"+ edgeDirection +\") are: \" + x._2.mkString(\",\") )\n",
    "    );\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mprintVertices\u001b[39m"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def printVertices[V, E](graph:Graph[_, _]):Unit = {\n",
    "    graph.vertices.map(\n",
    "      vd => \"Vertex ID = \" + vd._1 + \": \" + vd._2\n",
    "    ).collect.foreach(println(_))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (0,(0,0)) --[1.0]--> (1,(0,1))\n",
      " (0,(0,0)) --[1.0]--> (4,(1,0))\n",
      " (1,(0,1)) --[1.0]--> (2,(0,2))\n",
      " (1,(0,1)) --[1.0]--> (5,(1,1))\n",
      " (2,(0,2)) --[1.0]--> (3,(0,3))\n",
      " (2,(0,2)) --[1.0]--> (6,(1,2))\n",
      " (3,(0,3)) --[1.0]--> (7,(1,3))\n",
      " (4,(1,0)) --[1.0]--> (5,(1,1))\n",
      " (4,(1,0)) --[1.0]--> (8,(2,0))\n",
      " (5,(1,1)) --[1.0]--> (6,(1,2))\n",
      " (5,(1,1)) --[1.0]--> (9,(2,1))\n",
      " (6,(1,2)) --[1.0]--> (7,(1,3))\n",
      " (6,(1,2)) --[1.0]--> (10,(2,2))\n",
      " (7,(1,3)) --[1.0]--> (11,(2,3))\n",
      " (8,(2,0)) --[1.0]--> (9,(2,1))\n",
      " (8,(2,0)) --[1.0]--> (12,(3,0))\n",
      " (9,(2,1)) --[1.0]--> (10,(2,2))\n",
      " (9,(2,1)) --[1.0]--> (13,(3,1))\n",
      " (10,(2,2)) --[1.0]--> (11,(2,3))\n",
      " (10,(2,2)) --[1.0]--> (14,(3,2))\n",
      " (11,(2,3)) --[1.0]--> (15,(3,3))\n",
      " (12,(3,0)) --[1.0]--> (13,(3,1))\n",
      " (13,(3,1)) --[1.0]--> (14,(3,2))\n",
      " (14,(3,2)) --[1.0]--> (15,(3,3))\n"
     ]
    }
   ],
   "source": [
    "printEdges(graph3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (1,1) --[1]--> (0,1)\n",
      " (2,1) --[1]--> (0,1)\n",
      " (3,1) --[1]--> (0,1)\n",
      " (4,1) --[1]--> (0,1)\n",
      " (5,1) --[1]--> (0,1)\n",
      " (6,1) --[1]--> (0,1)\n",
      " (7,1) --[1]--> (0,1)\n"
     ]
    }
   ],
   "source": [
    "printEdges(graph4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Out-of-the-box graph algorithms in GraphX\n",
    "\n",
    "### Page Rank Algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/02 19:03:10 INFO Pregel: Pregel finished iteration 0\n",
      "23/06/02 19:03:10 INFO Pregel: Pregel finished iteration 1\n",
      "23/06/02 19:03:10 INFO Pregel: Pregel finished iteration 2\n",
      "23/06/02 19:03:10 INFO Pregel: Pregel finished iteration 3\n",
      "23/06/02 19:03:10 INFO Pregel: Pregel finished iteration 4\n",
      "23/06/02 19:03:10 INFO Pregel: Pregel finished iteration 5\n",
      "23/06/02 19:03:10 INFO Pregel: Pregel finished iteration 6\n",
      "23/06/02 19:03:11 INFO Pregel: Pregel finished iteration 7\n",
      "23/06/02 19:03:11 INFO Pregel: Pregel finished iteration 8\n",
      "23/06/02 19:03:11 INFO Pregel: Pregel finished iteration 9\n",
      "23/06/02 19:03:11 INFO Pregel: Pregel finished iteration 10\n",
      "23/06/02 19:03:11 INFO Pregel: Pregel finished iteration 11\n",
      "23/06/02 19:03:11 INFO Pregel: Pregel finished iteration 12\n",
      "23/06/02 19:03:11 INFO Pregel: Pregel finished iteration 13\n",
      "23/06/02 19:03:11 INFO Pregel: Pregel finished iteration 14\n",
      "23/06/02 19:03:11 INFO Pregel: Pregel finished iteration 15\n",
      "23/06/02 19:03:11 INFO Pregel: Pregel finished iteration 16\n",
      "23/06/02 19:03:11 INFO Pregel: Pregel finished iteration 17\n",
      "23/06/02 19:03:12 INFO Pregel: Pregel finished iteration 18\n",
      "23/06/02 19:03:12 INFO Pregel: Pregel finished iteration 19\n",
      "23/06/02 19:03:12 INFO Pregel: Pregel finished iteration 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph10\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mDouble\u001b[39m, \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@44faea2a"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph10 = graph1.pageRank(tol=0.01, resetProb = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/02 19:03:12 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "23/06/02 19:03:12 INFO SharedState: Warehouse path is 'file:/mnt/src/spark_projs/graphx-algorithms/examples/spark-warehouse'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mpgrank_df\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Vertex_id: bigint, pagerank_score: double]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pgrank_df = spark.createDataFrame(graph10.vertices)\n",
    "    .toDF(Seq(\"Vertex_id\", \"pagerank_score\"):_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n",
      "|Vertex_id|     pagerank_score|\n",
      "+---------+-------------------+\n",
      "|       80| 0.9822563471669581|\n",
      "|       30| 1.7297158594552773|\n",
      "|       50| 0.2880277933777645|\n",
      "|       40|0.15569069912311598|\n",
      "|       90| 0.9822563471669581|\n",
      "|       70| 0.9822563471669581|\n",
      "|       20|  1.862052953709926|\n",
      "|       60| 0.4005143234942158|\n",
      "|       10|  1.617229329338826|\n",
      "+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pgrank_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (10,1.617229329338826) --[1.0]--> (20,1.862052953709926)\n",
      " (20,1.862052953709926) --[1.0]--> (30,1.7297158594552773)\n",
      " (30,1.7297158594552773) --[1.0]--> (10,1.617229329338826)\n",
      " (70,0.9822563471669581) --[1.0]--> (80,0.9822563471669581)\n",
      " (40,0.15569069912311598) --[1.0]--> (50,0.2880277933777645)\n",
      " (50,0.2880277933777645) --[1.0]--> (60,0.4005143234942158)\n",
      " (60,0.4005143234942158) --[1.0]--> (20,1.862052953709926)\n",
      " (80,0.9822563471669581) --[1.0]--> (90,0.9822563471669581)\n",
      " (90,0.9822563471669581) --[1.0]--> (70,0.9822563471669581)\n"
     ]
    }
   ],
   "source": [
    "printEdges(graph10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connected Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/02 19:03:22 INFO Pregel: Pregel finished iteration 0\n",
      "23/06/02 19:03:22 INFO Pregel: Pregel finished iteration 1\n",
      "23/06/02 19:03:22 INFO Pregel: Pregel finished iteration 2\n",
      "23/06/02 19:03:22 INFO Pregel: Pregel finished iteration 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mccGraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mVertexId\u001b[39m, \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@a939132\n",
       "\u001b[36mvalidGraph\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mDouble\u001b[39m), \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@50431fc\n",
       "\u001b[36mvalidCCGraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mVertexId\u001b[39m, \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@53f27a28"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Run Connected Components\n",
    "val ccGraph = graph1.connectedComponents() // No longer contains missing field\n",
    "\n",
    "// Remove missing vertices as well as the edges to connected to them\n",
    "val validGraph = graph1.subgraph(vpred = (id, attr) => attr._2 != \"Missing\")\n",
    "\n",
    "// Restrict the answer to the valid subgraph\n",
    "val validCCGraph = ccGraph.mask(validGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (10,10) --[1.0]--> (20,10)\n",
      " (20,10) --[1.0]--> (30,10)\n",
      " (30,10) --[1.0]--> (10,10)\n",
      " (70,70) --[1.0]--> (80,70)\n",
      " (40,10) --[1.0]--> (50,10)\n",
      " (50,10) --[1.0]--> (60,10)\n",
      " (60,10) --[1.0]--> (20,10)\n",
      " (80,70) --[1.0]--> (90,70)\n",
      " (90,70) --[1.0]--> (70,70)\n"
     ]
    }
   ],
   "source": [
    "printEdges(ccGraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triangle counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtriGraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mInt\u001b[39m, \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@109b1f5f"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val triGraph = graph1.triangleCount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/02 19:03:24 WARN ShippableVertexPartitionOps: Joining two VertexPartitions with different indexes is slow.\n",
      "23/06/02 19:03:24 WARN ShippableVertexPartitionOps: Joining two VertexPartitions with different indexes is slow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (10,1) --[1.0]--> (20,1)\n",
      " (20,1) --[1.0]--> (30,1)\n",
      " (30,1) --[1.0]--> (10,1)\n",
      " (70,1) --[1.0]--> (80,1)\n",
      " (40,0) --[1.0]--> (50,0)\n",
      " (50,0) --[1.0]--> (60,0)\n",
      " (60,0) --[1.0]--> (20,1)\n",
      " (80,1) --[1.0]--> (90,1)\n",
      " (90,1) --[1.0]--> (70,1)\n"
     ]
    }
   ],
   "source": [
    "printEdges(triGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex ID = 80: 1\n",
      "Vertex ID = 30: 1\n",
      "Vertex ID = 50: 0\n",
      "Vertex ID = 40: 0\n",
      "Vertex ID = 90: 1\n",
      "Vertex ID = 70: 1\n",
      "Vertex ID = 20: 1\n",
      "Vertex ID = 60: 0\n",
      "Vertex ID = 10: 1\n"
     ]
    }
   ],
   "source": [
    "printVertices(triGraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strongly Connected Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/02 19:03:32 INFO Pregel: Pregel finished iteration 0\n",
      "23/06/02 19:03:32 INFO Pregel: Pregel finished iteration 1\n",
      "23/06/02 19:03:32 INFO Pregel: Pregel finished iteration 0\n",
      "23/06/02 19:03:32 INFO Pregel: Pregel finished iteration 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msccGraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mVertexId\u001b[39m, \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@6fdf83c"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sccGraph = graph1.stronglyConnectedComponents(numIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mscc_df\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Vertex_id: bigint, strong_conn_comp: bigint]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scc_df = spark.createDataFrame(sccGraph.vertices).toDF(Seq(\"Vertex_id\", \"strong_conn_comp\"):_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (10,10) --[1.0]--> (20,10)\n",
      " (20,10) --[1.0]--> (30,10)\n",
      " (30,10) --[1.0]--> (10,10)\n",
      " (70,70) --[1.0]--> (80,70)\n",
      " (40,40) --[1.0]--> (50,50)\n",
      " (50,50) --[1.0]--> (60,60)\n",
      " (60,60) --[1.0]--> (20,10)\n",
      " (80,70) --[1.0]--> (90,70)\n",
      " (90,70) --[1.0]--> (70,70)\n"
     ]
    }
   ],
   "source": [
    "printEdges(sccGraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Graph Algorithm output from the Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcc_df\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Vertex_id: bigint, Connected_id: bigint]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cc_df = spark.createDataFrame(ccGraph.vertices).toDF(Seq(\"Vertex_id\", \"Connected_id\"):_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|Vertex_id|Connected_id|\n",
      "+---------+------------+\n",
      "|       80|          70|\n",
      "|       30|          10|\n",
      "|       50|          10|\n",
      "|       40|          10|\n",
      "|       90|          70|\n",
      "|       70|          70|\n",
      "|       20|          10|\n",
      "|       60|          10|\n",
      "|       10|          10|\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cc_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtri_df\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Vertex_id: bigint, triangle_count: int]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tri_df = spark.createDataFrame(triGraph.vertices).toDF(Seq(\"Vertex_id\", \"triangle_count\"):_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph1_vtx_data\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Vertex_id: bigint, Connected_id: bigint ... 3 more fields]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// join dataframes on Vertex_id : pgrank_df, cc_df, tri_df, scc_df\n",
    "val graph1_vtx_data = cc_df.join(pgrank_df,cc_df(\"Vertex_id\") === pgrank_df(\"Vertex_id\"),\"inner\" )\n",
    ".join(tri_df,cc_df(\"Vertex_id\") === tri_df(\"Vertex_id\"),\"inner\" )\n",
    ".join(scc_df,cc_df(\"Vertex_id\") === scc_df(\"Vertex_id\"),\"inner\" )\n",
    ".select(cc_df(\"Vertex_id\"),cc_df(\"Connected_id\"),pgrank_df(\"pagerank_score\"), tri_df(\"triangle_count\"), scc_df(\"strong_conn_comp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/02 19:03:39 INFO ShufflePartitionsUtil: For shuffle(152, 153), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "23/06/02 19:03:39 INFO ShufflePartitionsUtil: For shuffle(154, 155), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-------------------+--------------+----------------+\n",
      "|Vertex_id|Connected_id|     pagerank_score|triangle_count|strong_conn_comp|\n",
      "+---------+------------+-------------------+--------------+----------------+\n",
      "|       10|          10|  1.617229329338826|             1|              10|\n",
      "|       20|          10|  1.862052953709926|             1|              10|\n",
      "|       30|          10| 1.7297158594552773|             1|              10|\n",
      "|       40|          10|0.15569069912311598|             0|              40|\n",
      "|       50|          10| 0.2880277933777645|             0|              50|\n",
      "|       60|          10| 0.4005143234942158|             0|              60|\n",
      "|       70|          70| 0.9822563471669581|             1|              70|\n",
      "|       80|          70| 0.9822563471669581|             1|              70|\n",
      "|       90|          70| 0.9822563471669581|             1|              70|\n",
      "+---------+------------+-------------------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph1_vtx_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/02 19:03:41 INFO ShufflePartitionsUtil: For shuffle(156, 157), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "23/06/02 19:03:41 INFO ShufflePartitionsUtil: For shuffle(158, 159), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "23/06/02 19:03:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/02 19:03:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/02 19:03:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/06/02 19:03:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/02 19:03:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/02 19:03:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/06/02 19:03:41 INFO FileOutputCommitter: Saved output of task 'attempt_20230602190341306750394207902040_2818_m_000000_470' to file:/tmp/graph1_properties.csv/_temporary/0/task_20230602190341306750394207902040_2818_m_000000\n",
      "23/06/02 19:03:41 INFO FileFormatWriter: Start to commit write Job 619a4453-b275-4342-b1f5-0bfc413e68eb.\n",
      "23/06/02 19:03:41 INFO FileFormatWriter: Write Job 619a4453-b275-4342-b1f5-0bfc413e68eb committed. Elapsed time: 25 ms.\n",
      "23/06/02 19:03:41 INFO FileFormatWriter: Finished processing stats for write job 619a4453-b275-4342-b1f5-0bfc413e68eb.\n"
     ]
    }
   ],
   "source": [
    "graph1_vtx_data.write\n",
    "    .option(\"header\",true)\n",
    "    .mode(SaveMode.Overwrite)\n",
    "    .csv(\"/tmp/graph1_properties.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple graph algorithm using the Aggregate Messages functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "// The syntax of the send message/aggregate message methods are:\n",
    "// val vrdd = graph1.aggregateMessages[Msg: ClassTag](\n",
    "//       sendMsg: EdgeContext[VD, ED, Msg] => Unit,\n",
    "//       mergeMsg: (Msg, Msg) => Msg,\n",
    "//       tripletFields: TripletFields = TripletFields.All)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this functionality, we can iteratively send messages and compute some simple metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres140\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mVertexId\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m((\u001b[32m2L\u001b[39m, \u001b[32m1\u001b[39m), (\u001b[32m3L\u001b[39m, \u001b[32m1\u001b[39m), (\u001b[32m10L\u001b[39m, \u001b[32m2\u001b[39m), (\u001b[32m5L\u001b[39m, \u001b[32m2\u001b[39m))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// get outDegrees of each edge by sending message of \"1\" from each edge to each source vertex\n",
    "// then, reduce these by adding\n",
    "// collect to local array to see vertex id with outdegree count\n",
    "graph2.aggregateMessages[Int](_.sendToSrc(1), _ + _).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres141\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mVertexId\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m((\u001b[32m1L\u001b[39m, \u001b[32m1\u001b[39m), (\u001b[32m7L\u001b[39m, \u001b[32m2\u001b[39m), (\u001b[32m3L\u001b[39m, \u001b[32m1\u001b[39m), (\u001b[32m5L\u001b[39m, \u001b[32m2\u001b[39m))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// now, repeat same for counting in-degrees, but by sending messages to each destination:\n",
    "graph2.aggregateMessages[Int](_.sendToDst(1), _ + _).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define custom functions for these simple aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msendMsgDestination\u001b[39m"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sendMsgDestination(e:EdgeContext[_, _, Int]):Unit = {\n",
    "    e.sendToDst(1)\n",
    "    // e.sendToSrc(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmergeMsgMethod\u001b[39m"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mergeMsgMethod(x:Int, y:Int):Int = {\n",
    "    x + y\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres144\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mDouble\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m80L\u001b[39m, \u001b[32m0.0\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m30L\u001b[39m, \u001b[32m0.0\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m50L\u001b[39m, \u001b[32m0.0\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m40L\u001b[39m, \u001b[32m0.0\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m90L\u001b[39m, \u001b[32m0.0\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m70L\u001b[39m, \u001b[32m0.0\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m20L\u001b[39m, \u001b[32m0.0\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m60L\u001b[39m, \u001b[32m0.0\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m10L\u001b[39m, \u001b[32m0.0\u001b[39m, \u001b[32m1\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in-degrees for graph1, but now using the defined functions\n",
    "graph1.aggregateMessages[Int](\n",
    "    sendMsgDestination(_), mergeMsgMethod(_, _)\n",
    ").rightOuterJoin(graph1.vertices)\n",
    ".map(x => (x._2._2._1, x._2._2._2, x._2._1.getOrElse(1)))\n",
    ".collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres145\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mInt\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m3\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m3\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m3\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph3.aggregateMessages[Int](\n",
    "    sendMsgDestination(_), mergeMsgMethod(_, _)\n",
    ").rightOuterJoin(graph3.vertices)\n",
    ".map(x => (x._2._2._1, x._2._2._2, x._2._1.getOrElse(1)))\n",
    ".collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres146\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mVertexId\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m0L\u001b[39m, \u001b[32m7\u001b[39m),\n",
       "  (\u001b[32m6L\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m1L\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m7L\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m2L\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m3L\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m4L\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m5L\u001b[39m, \u001b[32m0\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph4.aggregateMessages[Int](\n",
    "    sendMsgDestination(_), mergeMsgMethod(_, _)\n",
    ").rightOuterJoin(graph4.vertices)\n",
    ".map(x => (x._1, x._2._1.getOrElse(0)))\n",
    ".collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres147\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mVertexId\u001b[39m, \u001b[32mInt\u001b[39m, \u001b[32mLong\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m0L\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m4L\u001b[39m),\n",
       "  (\u001b[32m6L\u001b[39m, \u001b[32m7\u001b[39m, \u001b[32m1L\u001b[39m),\n",
       "  (\u001b[32m1L\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2L\u001b[39m),\n",
       "  (\u001b[32m7L\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m6L\u001b[39m),\n",
       "  (\u001b[32m8L\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m0L\u001b[39m),\n",
       "  (\u001b[32m2L\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m3L\u001b[39m),\n",
       "  (\u001b[32m3L\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m3L\u001b[39m),\n",
       "  (\u001b[32m9L\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m6L\u001b[39m),\n",
       "  (\u001b[32m4L\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m5L\u001b[39m),\n",
       "  (\u001b[32m5L\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m8L\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph5.aggregateMessages[Int](\n",
    "    sendMsgDestination(_), mergeMsgMethod(_, _)\n",
    ").rightOuterJoin(graph5.vertices)\n",
    ".map(x => (x._1, x._2._1.getOrElse(0), x._2._2))\n",
    ".collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (0,4) --[1]--> (0,4)\n",
      " (0,4) --[1]--> (0,4)\n",
      " (0,4) --[1]--> (5,8)\n",
      " (0,4) --[1]--> (9,6)\n",
      " (1,2) --[1]--> (1,2)\n",
      " (1,2) --[1]--> (8,0)\n",
      " (2,3) --[1]--> (6,1)\n",
      " (2,3) --[1]--> (6,1)\n",
      " (2,3) --[1]--> (7,6)\n",
      " (3,3) --[1]--> (2,3)\n",
      " (3,3) --[1]--> (4,5)\n",
      " (3,3) --[1]--> (6,1)\n",
      " (4,5) --[1]--> (3,3)\n",
      " (4,5) --[1]--> (3,3)\n",
      " (4,5) --[1]--> (3,3)\n",
      " (4,5) --[1]--> (6,1)\n",
      " (4,5) --[1]--> (8,0)\n",
      " (5,8) --[1]--> (2,3)\n",
      " (5,8) --[1]--> (2,3)\n",
      " (5,8) --[1]--> (4,5)\n",
      " (5,8) --[1]--> (4,5)\n",
      " (5,8) --[1]--> (6,1)\n",
      " (5,8) --[1]--> (8,0)\n",
      " (5,8) --[1]--> (8,0)\n",
      " (5,8) --[1]--> (9,6)\n",
      " (6,1) --[1]--> (4,5)\n",
      " (7,6) --[1]--> (0,4)\n",
      " (7,6) --[1]--> (3,3)\n",
      " (7,6) --[1]--> (6,1)\n",
      " (7,6) --[1]--> (8,0)\n",
      " (7,6) --[1]--> (9,6)\n",
      " (7,6) --[1]--> (9,6)\n",
      " (9,6) --[1]--> (0,4)\n",
      " (9,6) --[1]--> (2,3)\n",
      " (9,6) --[1]--> (3,3)\n",
      " (9,6) --[1]--> (5,8)\n",
      " (9,6) --[1]--> (6,1)\n",
      " (9,6) --[1]--> (9,6)\n"
     ]
    }
   ],
   "source": [
    "printEdges(graph5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Simple Example of using the Pregel API\n",
    "\n",
    "Pregel provides an iterative graph-parallel computation mechanism comprising of multiple supersteps.\n",
    "\n",
    "It requires specifying three functions:\n",
    "\n",
    "  1. Method to update a vertex given its incoming messages\n",
    "  2. Method to send message based on a defined logic\n",
    "  3. Method to merge messages received at a given vertex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "// val graph11 = graph1.pregel[A](initialMsg: A, maxIterations: Int, activeDirection: EdgeDirection)(\n",
    "//       vprog: (VertexId, VD, A) => VD,\n",
    "//       sendMsg: EdgeTriplet[VD, ED] => Iterator[(VertexId, A)],\n",
    "//       mergeMsg: (A, A) => A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define these functions to use them for a simple pregel algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mvertexProgram\u001b[39m"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// assume message 'A' is a Int\n",
    "// update vertex data by adding incoming message value with vertex ID\n",
    "def vertexProgram(vid: VertexId, existingVtxVal:Int , messageVal:Int ): Int  = {\n",
    "    existingVtxVal + messageVal\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msendMsgMethod\u001b[39m"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// send message only if:\n",
    "// the current vertex value + edge value/distance is less than destination vertex value\n",
    "def sendMsgMethod[VD, ED](triplet:EdgeTriplet[VertexId, VertexId]):Iterator[_] = {\n",
    "    if (triplet.srcAttr + triplet.attr < triplet.dstAttr) {\n",
    "      Iterator((triplet.dstId, triplet.srcAttr + triplet.attr))\n",
    "    } else {\n",
    "      Iterator.empty\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmergeMsgMethod\u001b[39m"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mergeMsgMethod(x:Int, y:Int):Int = {\n",
    "    x + y\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mexampleVerticesResult\u001b[39m: \u001b[32mVertexRDD\u001b[39m[\u001b[32mInt\u001b[39m] = VertexRDDImpl[1147] at RDD at VertexRDD.scala:57"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val exampleVerticesResult = graph3.aggregateMessages[Int](\n",
    "    sendMsgDestination(_), mergeMsgMethod(_, _)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres153\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mVertexId\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m6L\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m12L\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m13L\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m1L\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m7L\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m14L\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m8L\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m2L\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m15L\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m3L\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m9L\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m4L\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m10L\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m11L\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m5L\u001b[39m, \u001b[32m2\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exampleVerticesResult.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (0,(0,0)) --[1.0]--> (1,(0,1))\n",
      " (0,(0,0)) --[1.0]--> (4,(1,0))\n",
      " (1,(0,1)) --[1.0]--> (2,(0,2))\n",
      " (1,(0,1)) --[1.0]--> (5,(1,1))\n",
      " (2,(0,2)) --[1.0]--> (3,(0,3))\n",
      " (2,(0,2)) --[1.0]--> (6,(1,2))\n",
      " (3,(0,3)) --[1.0]--> (7,(1,3))\n",
      " (4,(1,0)) --[1.0]--> (5,(1,1))\n",
      " (4,(1,0)) --[1.0]--> (8,(2,0))\n",
      " (5,(1,1)) --[1.0]--> (6,(1,2))\n",
      " (5,(1,1)) --[1.0]--> (9,(2,1))\n",
      " (6,(1,2)) --[1.0]--> (7,(1,3))\n",
      " (6,(1,2)) --[1.0]--> (10,(2,2))\n",
      " (7,(1,3)) --[1.0]--> (11,(2,3))\n",
      " (8,(2,0)) --[1.0]--> (9,(2,1))\n",
      " (8,(2,0)) --[1.0]--> (12,(3,0))\n",
      " (9,(2,1)) --[1.0]--> (10,(2,2))\n",
      " (9,(2,1)) --[1.0]--> (13,(3,1))\n",
      " (10,(2,2)) --[1.0]--> (11,(2,3))\n",
      " (10,(2,2)) --[1.0]--> (14,(3,2))\n",
      " (11,(2,3)) --[1.0]--> (15,(3,3))\n",
      " (12,(3,0)) --[1.0]--> (13,(3,1))\n",
      " (13,(3,1)) --[1.0]--> (14,(3,2))\n",
      " (14,(3,2)) --[1.0]--> (15,(3,3))\n"
     ]
    }
   ],
   "source": [
    "printEdges(graph3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Applying the Pregel API in a simple algorithm\n",
    "\n",
    "Here is an example of computing the single source shortest path in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mShortestPath\u001b[39m"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ShortestPath(graph: Graph[(Long, Double), Double], srcID: VertexId): RDD[Row] = {\n",
    "    \n",
    "    // Initialize the graph such that all vertices except the root have distance infinity.\n",
    "    val initialGraph = graph.mapVertices(\n",
    "        (id, _) =>\n",
    "        if (id == srcID) 0.0 else Double.PositiveInfinity\n",
    "    )\n",
    "\n",
    "    val sssp = initialGraph.pregel(Double.PositiveInfinity, maxIterations=7)(\n",
    "      (id, dist, newDist) => math.min(dist, newDist), // Vertex Program\n",
    "      triplet => {  // Send Message\n",
    "        if (triplet.srcAttr + triplet.attr < triplet.dstAttr) {\n",
    "          Iterator((triplet.dstId, triplet.srcAttr + triplet.attr))\n",
    "        } else {\n",
    "          Iterator.empty\n",
    "        }\n",
    "      },\n",
    "      (a, b) => math.min(a, b) // Merge Message\n",
    "    )\n",
    "    \n",
    "    return( sssp.vertices.filter(y => y._2 < Double.PositiveInfinity).map( x => Row(srcID, x._1, x._2)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/02 19:04:22 INFO Pregel: Pregel finished iteration 0\n",
      "23/06/02 19:04:22 INFO Pregel: Pregel finished iteration 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdistances\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mRow\u001b[39m] = MapPartitionsRDD[1203] at map at cmd155.sc:21"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val distances = ShortestPath(graph1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from Src node (10) to Destination node =\n",
      "[Lorg.apache.spark.sql.Row;@570826fc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrowArray\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mRow\u001b[39m] = \u001b[33mArray\u001b[39m([10,30,2.0], [10,20,1.0], [10,10,0.0])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rowArray = distances.collect()\n",
    "println(\"Distance from Src node (10) to Destination node =\")\n",
    "println(rowArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from Src node (40) to Destination node =\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/02 19:06:01 INFO Pregel: Pregel finished iteration 0\n",
      "23/06/02 19:06:01 INFO Pregel: Pregel finished iteration 1\n",
      "23/06/02 19:06:01 INFO Pregel: Pregel finished iteration 2\n",
      "23/06/02 19:06:01 INFO Pregel: Pregel finished iteration 3\n",
      "23/06/02 19:06:02 INFO Pregel: Pregel finished iteration 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40,30,4.0]\n",
      "[40,50,1.0]\n",
      "[40,40,0.0]\n",
      "[40,20,3.0]\n",
      "[40,60,2.0]\n",
      "[40,10,5.0]\n"
     ]
    }
   ],
   "source": [
    "println(\"Distance from Src node (40) to Destination node =\")\n",
    "println(ShortestPath(graph1, 40).collect().mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Stop the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/02 19:06:06 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
