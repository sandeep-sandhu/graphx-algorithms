{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Algorithms - Part 1 - The Basics\n",
    "\n",
    "## Implemented in the Apache Spark GraphX platform\n",
    "\n",
    "Topics Covered:\n",
    "\n",
    "  1. Initialising the environment\n",
    "  1. Starting a Spark session\n",
    "  1. Load sample graph data\n",
    "  1. Basic GraphX operations\n",
    "  1. Simple Pregel API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Initialising the environment\n",
    "\n",
    "### 1.1 Source the libraries for Apache Spark\n",
    "\n",
    "When running in a jupyter notebook, sometimes the required libraries may not exist in the classpath.\n",
    "\n",
    "Load essential spark libraries from maven public repositories at runtime like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "a7ca08d823f240129c8c3b8e1deb4fa2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                          \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                        \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-core:3.2.0`\n",
    "import $ivy.`org.apache.spark::spark-mllib-local:3.2.0`\n",
    "import $ivy.`org.apache.spark::spark-mllib:3.2.0`\n",
    "import $ivy.`org.apache.spark::spark-graphx:3.2.0`\n",
    "import $ivy.`org.apache.spark::spark-streaming:3.2.0`\n",
    "import $ivy.`org.apache.spark::spark-tags:3.2.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.scalanlp::breeze-viz:1.2`\n",
    "import $ivy.`org.jfree:jfreechart:1.5.4`\n",
    "import $ivy.`org.creativescala::doodle-core:0.9.21`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.2 Import the Spark Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "732e8d55868d4b848e8a1aa50e5a9a12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cc8495eee18e4c97abe4af3ac3e87209"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.linalg.{Matrix, Vectors}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Row\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Dataset\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.{col, udf, _}\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.{Matrix, Vectors}\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.functions.{col, udf, _}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ec63a53bf3c4458984c0ab716e5aa0b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.graphx._\n",
       "// To make some of the examples work we will also need RDD\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.rdd.RDD\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.graphx._\n",
    "// To make some of the examples work we will also need RDD\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mbreeze.linalg._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mbreeze.plot._\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import breeze.linalg._\n",
    "import breeze.plot._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "// this uses the IBM DB2 connector to read from a DB2 table\n",
    "//import $ivy.`com.ibm.db2.jcc:db2jcc:db2jcc4`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bbf5510bb79c40b18ded72a586e03f7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mappName\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Spark_Graph_Algorithms\"\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val appName = \"Spark_Graph_Algorithms\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Setup the Logger\n",
    "\n",
    "To control the volume of log messages, change the log4j configuraiton programatically like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9546af49dfa44cebaca97724c6add0b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "//Logger.getLogger(\"org\").setLevel(Level.INFO)\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mlogger\u001b[39m: \u001b[32mLogger\u001b[39m = org.apache.log4j.Logger@3463b322"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "//Logger.getLogger(\"org\").setLevel(Level.INFO)\n",
    "\n",
    "val logger: Logger = Logger.getLogger(appName)\n",
    "Logger.getLogger(\"org.apache.spark\").setLevel(Level.WARN)\n",
    "Logger.getLogger(\"org.apache.spark.storage.BlockManager\").setLevel(Level.ERROR)\n",
    "logger.setLevel(Level.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Create Spark session\n",
    "\n",
    "### 2.1 Initialise Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "// close the spark session and spark context before starting a new one, if re-executing the notebook.\n",
    "\n",
    "//spark.stop()\n",
    "//sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkConf\u001b[39m: \u001b[32mSparkConf\u001b[39m = org.apache.spark.SparkConf@6fb026"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sparkConf = new SparkConf()\n",
    "             .setAppName(appName)\n",
    "             .setMaster(\"local[*]\")\n",
    "             //.setMaster(\"spark://localhost:7077\")\n",
    "             //.setMaster(\"spark://sparkmaster320:7077\")\n",
    "             .set(\"spark.driver.extraClassPath\", \"c:/bin/lib/db2jcc4.jar,c:/bin/lib/breeze-viz_2.12-1.2.jar\")\n",
    "             .set(\"spark.executor.extraClassPath\", \"c:/bin/lib/db2jcc4.jar,c:/bin/lib/breeze-viz_2.12-1.2.jar\")\n",
    "             .set(\"spark.default.parallelism\", \"6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1e211dd28ca04c588d837d12902c7af8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "23/05/21 10:24:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@23dcaed"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Apply the config to start a spark session:\n",
    "val spark = org.apache.spark.sql.SparkSession.builder()\n",
    "    .config(sparkConf)\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "42ddce31bb4249bc89cc454a78399602"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msc\u001b[39m: \u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@deaff9a"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Get information on Spark Session\n",
    "\n",
    "Use spark context and config objects to get essential information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1ac76653ab504ac3972b3e7df13eff68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Master: local[*], User: notebooker, Version: 3.2.0, Deployment mode: client\n",
      "Default Partitions: 2, Scheduling Mode: FIFO\n"
     ]
    }
   ],
   "source": [
    "println(\"Spark Master: %s, User: %s, Version: %s, Deployment mode: %s\".format(\n",
    "        sc.master, sc.sparkUser, sc.version, sc.deployMode\n",
    "    ))\n",
    "\n",
    "println(\"Default Partitions: %d, Scheduling Mode: %s\".format(\n",
    "         sc.defaultMinPartitions, sc.getSchedulingMode\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "e891aa7630ef48f8898d5eb853111ac3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Parameter: spark.driver.host=jupyterlab\n",
      "Configuration Parameter: spark.app.startTime=1684644882273\n",
      "Configuration Parameter: spark.driver.port=38353\n",
      "Configuration Parameter: spark.default.parallelism=6\n",
      "Configuration Parameter: spark.executor.extraClassPath=c:/bin/lib/db2jcc4.jar,c:/bin/lib/breeze-viz_2.12-1.2.jar\n",
      "Configuration Parameter: spark.app.name=Spark_Graph_Algorithms\n",
      "Configuration Parameter: spark.master=local[*]\n",
      "Configuration Parameter: spark.driver.extraClassPath=c:/bin/lib/db2jcc4.jar,c:/bin/lib/breeze-viz_2.12-1.2.jar\n",
      "Configuration Parameter: spark.executor.id=driver\n",
      "Configuration Parameter: spark.app.id=local-1684644889897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mconfig\u001b[39m: \u001b[32mSparkConf\u001b[39m = org.apache.spark.SparkConf@64832105"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val config = sc.getConf\n",
    "\n",
    "for ((k,v) <- config.getAll) println(s\"Configuration Parameter: $k=$v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres13\u001b[39m: \u001b[32mOption\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mSome\u001b[39m(\n",
       "  \u001b[32m\"c:/bin/lib/db2jcc4.jar,c:/bin/lib/breeze-viz_2.12-1.2.jar\"\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.getOption(\"spark.executor.extraClassPath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "73685a5331b447fe9921a52c8d154e43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres14\u001b[39m: \u001b[32mOption\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[32mNone\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.getOption(\"spark.jars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "937b84b1044e4062a1522e11f77f37d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres15\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/opt/conda/bin:/home/notebooker/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\"\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.env(\"PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// For implicit conversions like converting RDDs to DataFrames\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.storage.StorageLevel\u001b[39m"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.storage.StorageLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load sample graph data\n",
    "\n",
    "Data can be loaded into a graph by reading from an edgelist file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/22 10:50:36 INFO FileInputFormat: Total input files to process : 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph1\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mDouble\u001b[39m), \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@5ace36c4"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// read from edgelist file\n",
    "val graph1 = GraphLoader\n",
    "      .edgeListFile(sc,\n",
    "                    \"../src/test/resources/graph1_edgelist.txt\",\n",
    "                    edgeStorageLevel=StorageLevel.MEMORY_AND_DISK,\n",
    "                    vertexStorageLevel=StorageLevel.MEMORY_AND_DISK)\n",
    "      .mapEdges(e => e.attr.toDouble)\n",
    "      .mapVertices[(Long, Double)]((vid, data) => (vid.toLong, 0.0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can also be fed in via RDDs of edges and vertices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36musers\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mVertexId\u001b[39m, (\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m))] = ParallelCollectionRDD[210] at parallelize at cmd64.sc:2"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create an RDD for the vertices\n",
    "val users: RDD[(VertexId, (String, String))] =\n",
    "  sc.parallelize( Array(\n",
    "        (3L, (\"rxin\", \"student\"))\n",
    "      , (7L, (\"jgonzal\", \"postdoc\"))\n",
    "      , (1L, (\"somebody\", \"postdoc\"))\n",
    "      , (5L, (\"franklin\", \"prof\"))\n",
    "      , (2L, (\"istoica\", \"prof\"))\n",
    "      , (10L, (\"hoityToity\", \"student\"))\n",
    "     )\n",
    "   ).persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrelationships\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mEdge\u001b[39m[\u001b[32mString\u001b[39m]] = ParallelCollectionRDD[209] at parallelize at cmd63.sc:2"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create an RDD for edges\n",
    "val relationships: RDD[Edge[String]] =\n",
    "  sc.parallelize(\n",
    "      Array(\n",
    "      Edge(3L, 7L, \"collab\")\n",
    "      , Edge(5L, 3L, \"advisor\")\n",
    "      , Edge(2L, 5L, \"colleague\")\n",
    "      , Edge(5L, 7L, \"pi\")\n",
    "      , Edge(10L, 5L, \"friend\")\n",
    "      , Edge(10L, 1L, \"friend\")\n",
    "      )\n",
    "    ).persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdefaultUser\u001b[39m: (\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m) = (\u001b[32m\"John Doe\"\u001b[39m, \u001b[32m\"Missing\"\u001b[39m)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Define a default user in case there are relationship with missing user\n",
    "val defaultUser = (\"John Doe\", \"Missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph2\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m), \u001b[32mString\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@58cdbc18"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Build the initial Graph\n",
    "val graph2 = Graph(users, relationships, defaultUser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Basic GraphX operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mprintAllEdges\u001b[39m"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define convenience function to print all edges of a graph:\n",
    "def printAllEdges[V, D, E](graph: Graph[(V, D), E]): Unit = {\n",
    "\n",
    "val facts: RDD[String] = graph.triplets.map(\n",
    "  triplet =>\n",
    "  \"(\" + triplet.srcId + \",\" + triplet.srcAttr._1 + \") --[ \" + triplet.attr + \" ]--> (\" + triplet.dstId + \",\" + triplet.dstAttr._1 + \")\");\n",
    "\n",
    "facts.collect.foreach(println(_))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mprintOnlyEdges\u001b[39m"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define convenience function to print all edges of a graph:\n",
    "def printOnlyEdges[V, E]( graph: Graph[V, E] ): Unit = {\n",
    "    \n",
    "    val facts: RDD[String] = graph.triplets.map(triplet => \n",
    "      \" \" + triplet.toTuple._1 + \" --[\" + triplet.toTuple._3 + \"]--> \" + triplet.toTuple._2 );\n",
    "\n",
    "    facts.collect.foreach(println(_))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mprintGraphProperties\u001b[39m"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def printGraphProperties( graph: Graph[_,_] ): Unit = {\n",
    "    // graph operators:\n",
    "    println( \"Num of edges = \" + graph.numEdges )\n",
    "    println( \"Num of vertices = \" + graph.numVertices )\n",
    "    println( \"Num of inDegrees = \" + graph.inDegrees.count() )\n",
    "    println( \"Num of outDegrees = \" + graph.outDegrees.count() )\n",
    "    println( \"Num of degrees = \" + graph.degrees.count() )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mprintNeighbors\u001b[39m"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def printNeighbors[V, D, E](graph: Graph[_, _], edgeDirection: EdgeDirection):Unit = {\n",
    "    graph.collectNeighborIds(edgeDirection).collect.foreach(\n",
    "      x =>\n",
    "        println(\"Neighbors of \" + x._1 + \" (\"+ edgeDirection +\") are: \" + x._2.mkString(\",\") )\n",
    "    );\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mprintVertices\u001b[39m"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def printVertices[V, E](graph:Graph[_, _]):Unit = {\n",
    "    graph.vertices.map(\n",
    "      vd => \"Vertex ID = \" + vd._1 + \": \" + vd._2\n",
    "    ).collect.foreach(println(_))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex ID = 1: (somebody,postdoc)\n",
      "Vertex ID = 7: (jgonzal,postdoc)\n",
      "Vertex ID = 2: (istoica,prof)\n",
      "Vertex ID = 3: (rxin,student)\n",
      "Vertex ID = 10: (hoityToity,student)\n",
      "Vertex ID = 5: (franklin,prof)\n"
     ]
    }
   ],
   "source": [
    "printVertices( graph2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,10) --[ 1.0 ]--> (20,20)\n",
      "(20,20) --[ 1.0 ]--> (30,30)\n",
      "(30,30) --[ 1.0 ]--> (10,10)\n",
      "(70,70) --[ 1.0 ]--> (80,80)\n",
      "(40,40) --[ 1.0 ]--> (50,50)\n",
      "(50,50) --[ 1.0 ]--> (60,60)\n",
      "(60,60) --[ 1.0 ]--> (20,20)\n",
      "(80,80) --[ 1.0 ]--> (90,90)\n",
      "(90,90) --[ 1.0 ]--> (70,70)\n"
     ]
    }
   ],
   "source": [
    "// print out the graph:\n",
    "printAllEdges( graph1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,rxin) --[ collab ]--> (7,jgonzal)\n",
      "(5,franklin) --[ advisor ]--> (3,rxin)\n",
      "(2,istoica) --[ colleague ]--> (5,franklin)\n",
      "(5,franklin) --[ pi ]--> (7,jgonzal)\n",
      "(10,hoityToity) --[ friend ]--> (5,franklin)\n",
      "(10,hoityToity) --[ friend ]--> (1,somebody)\n"
     ]
    }
   ],
   "source": [
    "// print out the graph:\n",
    "printAllEdges( graph2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of edges = 9\n",
      "Num of vertices = 9\n",
      "Num of inDegrees = 8\n",
      "Num of outDegrees = 9\n",
      "Num of degrees = 9\n"
     ]
    }
   ],
   "source": [
    "// print out basic properties:\n",
    "printGraphProperties(graph1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of edges = 6\n",
      "Num of vertices = 6\n",
      "Num of inDegrees = 4\n",
      "Num of outDegrees = 4\n",
      "Num of degrees = 6\n"
     ]
    }
   ],
   "source": [
    "// print out basic properties:\n",
    "printGraphProperties(graph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 1: Count all the edges where src > dst\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "println( \"Graph 1: Count all the edges where src > dst\")\n",
    "println( graph1.edges.filter(e => e.srcId > e.dstId).count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 2: Count all the edges where src > dst\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "println( \"Graph 2: Count all the edges where src > dst\")\n",
    "println( graph2.edges.filter(e => e.srcId > e.dstId).count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 1: Reversed edge directions\n",
      "(20,20) --[ 1.0 ]--> (10,10)\n",
      "(30,30) --[ 1.0 ]--> (20,20)\n",
      "(50,50) --[ 1.0 ]--> (40,40)\n",
      "(80,80) --[ 1.0 ]--> (70,70)\n",
      "(20,20) --[ 1.0 ]--> (60,60)\n",
      "(60,60) --[ 1.0 ]--> (50,50)\n",
      "(70,70) --[ 1.0 ]--> (90,90)\n",
      "(90,90) --[ 1.0 ]--> (80,80)\n"
     ]
    }
   ],
   "source": [
    "println( \"Graph 1: Reversed edge directions\")\n",
    "printAllEdges( graph1.reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph3\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mDouble\u001b[39m), \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@2744f861"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph3 = graph1.subgraph(vpred = (id, attr) => id > 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 1: subgraph where src vertex id > 10\n",
      "(20,20) --[ 1.0 ]--> (30,30)\n",
      "(40,40) --[ 1.0 ]--> (50,50)\n",
      "(70,70) --[ 1.0 ]--> (80,80)\n",
      "(50,50) --[ 1.0 ]--> (60,60)\n",
      "(60,60) --[ 1.0 ]--> (20,20)\n",
      "(80,80) --[ 1.0 ]--> (90,90)\n",
      "(90,90) --[ 1.0 ]--> (70,70)\n"
     ]
    }
   ],
   "source": [
    "println( \"Graph 1: subgraph where src vertex id > 10\")\n",
    "printAllEdges(graph3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph2_nofriends\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m), \u001b[32mString\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@3cdef54d"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph2_nofriends = graph2.subgraph(epred = edgetriplet => edgetriplet.attr != \"friend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph2_friends\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m), \u001b[32mString\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@2e30b00f"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph2_friends = graph2.subgraph(epred = edgetriplet => edgetriplet.attr == \"friend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,rxin) --[ collab ]--> (7,jgonzal)\n",
      "(5,franklin) --[ advisor ]--> (3,rxin)\n",
      "(2,istoica) --[ colleague ]--> (5,franklin)\n",
      "(5,franklin) --[ pi ]--> (7,jgonzal)\n"
     ]
    }
   ],
   "source": [
    "printAllEdges(graph2_nofriends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph5\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mDouble\u001b[39m), \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@2fcd897a"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph5 = graph1.mask(graph3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,20) --[ 1.0 ]--> (30,30)\n",
      "(40,40) --[ 1.0 ]--> (50,50)\n",
      "(70,70) --[ 1.0 ]--> (80,80)\n",
      "(50,50) --[ 1.0 ]--> (60,60)\n",
      "(60,60) --[ 1.0 ]--> (20,20)\n",
      "(80,80) --[ 1.0 ]--> (90,90)\n",
      "(90,90) --[ 1.0 ]--> (70,70)\n"
     ]
    }
   ],
   "source": [
    "printAllEdges(graph5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph6\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m), \u001b[32mString\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@6534b2f5"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph6 = graph2.groupEdges( (x, y) => \"friend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,rxin) --[ collab ]--> (7,jgonzal)\n",
      "(5,franklin) --[ advisor ]--> (3,rxin)\n",
      "(2,istoica) --[ colleague ]--> (5,franklin)\n",
      "(5,franklin) --[ pi ]--> (7,jgonzal)\n",
      "(10,hoityToity) --[ friend ]--> (5,franklin)\n",
      "(10,hoityToity) --[ friend ]--> (1,somebody)\n"
     ]
    }
   ],
   "source": [
    "printAllEdges(graph6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbors of 80 (EdgeDirection.Out) are: 90\n",
      "Neighbors of 30 (EdgeDirection.Out) are: 10\n",
      "Neighbors of 50 (EdgeDirection.Out) are: 60\n",
      "Neighbors of 40 (EdgeDirection.Out) are: 50\n",
      "Neighbors of 90 (EdgeDirection.Out) are: 70\n",
      "Neighbors of 70 (EdgeDirection.Out) are: 80\n",
      "Neighbors of 20 (EdgeDirection.Out) are: 30\n",
      "Neighbors of 60 (EdgeDirection.Out) are: 20\n",
      "Neighbors of 10 (EdgeDirection.Out) are: 20\n"
     ]
    }
   ],
   "source": [
    "printNeighbors(graph1, EdgeDirection.Out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbors of 1 (EdgeDirection.In) are: 10\n",
      "Neighbors of 7 (EdgeDirection.In) are: 3,5\n",
      "Neighbors of 2 (EdgeDirection.In) are: \n",
      "Neighbors of 3 (EdgeDirection.In) are: 5\n",
      "Neighbors of 10 (EdgeDirection.In) are: \n",
      "Neighbors of 5 (EdgeDirection.In) are: 2,10\n"
     ]
    }
   ],
   "source": [
    "printNeighbors(graph2, EdgeDirection.In)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Join RDDs with the graph\n",
    "val graph7 = graph1.joinVertices[U](table: RDD[(VertexId, U)])(mapFunc: (VertexId, VD, U) => VD)\n",
    "\n",
    "val graph8 = graph1.outerJoinVertices[U, VD2](other: RDD[(VertexId, U)])\n",
    "      (mapFunc: (VertexId, VD, Option[U]) => VD2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// send message\n",
    "val vrdd = graph1.aggregateMessages[Msg: ClassTag](\n",
    "      sendMsg: EdgeContext[VD, ED, Msg] => Unit,\n",
    "      mergeMsg: (Msg, Msg) => Msg,\n",
    "      tripletFields: TripletFields = TripletFields.All)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic graph algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph10\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mDouble\u001b[39m, \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@65d7b55a"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph10 = graph1.pageRank(tol=0.01, resetProb = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpgrank_df\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Vertex_id: bigint, pagerank_score: double]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pgrank_df = spark.createDataFrame(graph10.vertices).toDF(Seq(\"Vertex_id\", \"pagerank_score\"):_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n",
      "|Vertex_id|     pagerank_score|\n",
      "+---------+-------------------+\n",
      "|       80| 0.9822563471669581|\n",
      "|       30| 1.7297158594552773|\n",
      "|       50| 0.2880277933777645|\n",
      "|       40|0.15569069912311598|\n",
      "|       90| 0.9822563471669581|\n",
      "|       70| 0.9822563471669581|\n",
      "|       20|  1.862052953709926|\n",
      "|       60| 0.4005143234942158|\n",
      "|       10|  1.617229329338826|\n",
      "+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pgrank_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (10,1.617229329338826) --[1.0]--> (20,1.862052953709926)\n",
      " (20,1.862052953709926) --[1.0]--> (30,1.7297158594552773)\n",
      " (30,1.7297158594552773) --[1.0]--> (10,1.617229329338826)\n",
      " (70,0.9822563471669581) --[1.0]--> (80,0.9822563471669581)\n",
      " (40,0.15569069912311598) --[1.0]--> (50,0.2880277933777645)\n",
      " (50,0.2880277933777645) --[1.0]--> (60,0.4005143234942158)\n",
      " (60,0.4005143234942158) --[1.0]--> (20,1.862052953709926)\n",
      " (80,0.9822563471669581) --[1.0]--> (90,0.9822563471669581)\n",
      " (90,0.9822563471669581) --[1.0]--> (70,0.9822563471669581)\n"
     ]
    }
   ],
   "source": [
    "printOnlyEdges(graph10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex ID = 80: 0.9822563471669581\n",
      "Vertex ID = 30: 1.7297158594552773\n",
      "Vertex ID = 50: 0.2880277933777645\n",
      "Vertex ID = 40: 0.15569069912311598\n",
      "Vertex ID = 90: 0.9822563471669581\n",
      "Vertex ID = 70: 0.9822563471669581\n",
      "Vertex ID = 20: 1.862052953709926\n",
      "Vertex ID = 60: 0.4005143234942158\n",
      "Vertex ID = 10: 1.617229329338826\n"
     ]
    }
   ],
   "source": [
    "printVertices(graph10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mccGraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mVertexId\u001b[39m, \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@777bf690\n",
       "\u001b[36mvalidGraph\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mDouble\u001b[39m), \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@5143abf9\n",
       "\u001b[36mvalidCCGraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mVertexId\u001b[39m, \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@4e7b99f1"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Run Connected Components\n",
    "val ccGraph = graph1.connectedComponents() // No longer contains missing field\n",
    "\n",
    "// Remove missing vertices as well as the edges to connected to them\n",
    "val validGraph = graph1.subgraph(vpred = (id, attr) => attr._2 != \"Missing\")\n",
    "\n",
    "// Restrict the answer to the valid subgraph\n",
    "val validCCGraph = ccGraph.mask(validGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (10,10) --[1.0]--> (20,10)\n",
      " (20,10) --[1.0]--> (30,10)\n",
      " (30,10) --[1.0]--> (10,10)\n",
      " (70,70) --[1.0]--> (80,70)\n",
      " (40,10) --[1.0]--> (50,10)\n",
      " (50,10) --[1.0]--> (60,10)\n",
      " (60,10) --[1.0]--> (20,10)\n",
      " (80,70) --[1.0]--> (90,70)\n",
      " (90,70) --[1.0]--> (70,70)\n"
     ]
    }
   ],
   "source": [
    "printOnlyEdges(ccGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex ID = 80: 70\n",
      "Vertex ID = 30: 10\n",
      "Vertex ID = 50: 10\n",
      "Vertex ID = 40: 10\n",
      "Vertex ID = 90: 70\n",
      "Vertex ID = 70: 70\n",
      "Vertex ID = 20: 10\n",
      "Vertex ID = 60: 10\n",
      "Vertex ID = 10: 10\n"
     ]
    }
   ],
   "source": [
    "printVertices(ccGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtriGraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mInt\u001b[39m, \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@5eccb798"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val triGraph = graph1.triangleCount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/22 10:52:38 WARN ShippableVertexPartitionOps: Joining two VertexPartitions with different indexes is slow.\n",
      "23/05/22 10:52:38 WARN ShippableVertexPartitionOps: Joining two VertexPartitions with different indexes is slow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (10,1) --[1.0]--> (20,1)\n",
      " (20,1) --[1.0]--> (30,1)\n",
      " (30,1) --[1.0]--> (10,1)\n",
      " (70,1) --[1.0]--> (80,1)\n",
      " (40,0) --[1.0]--> (50,0)\n",
      " (50,0) --[1.0]--> (60,0)\n",
      " (60,0) --[1.0]--> (20,1)\n",
      " (80,1) --[1.0]--> (90,1)\n",
      " (90,1) --[1.0]--> (70,1)\n"
     ]
    }
   ],
   "source": [
    "printOnlyEdges(triGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex ID = 80: 1\n",
      "Vertex ID = 30: 1\n",
      "Vertex ID = 50: 0\n",
      "Vertex ID = 40: 0\n",
      "Vertex ID = 90: 1\n",
      "Vertex ID = 70: 1\n",
      "Vertex ID = 20: 1\n",
      "Vertex ID = 60: 0\n",
      "Vertex ID = 10: 1\n"
     ]
    }
   ],
   "source": [
    "printVertices(triGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcc_df\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Vertex_id: bigint, Connected_id: bigint]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cc_df = spark.createDataFrame(ccGraph.vertices).toDF(Seq(\"Vertex_id\", \"Connected_id\"):_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|Vertex_id|Connected_id|\n",
      "+---------+------------+\n",
      "|       80|          70|\n",
      "|       30|          10|\n",
      "|       50|          10|\n",
      "|       40|          10|\n",
      "|       90|          70|\n",
      "|       70|          70|\n",
      "|       20|          10|\n",
      "|       60|          10|\n",
      "|       10|          10|\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cc_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtri_df\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Vertex_id: bigint, triangle_count: int]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tri_df = spark.createDataFrame(triGraph.vertices).toDF(Seq(\"Vertex_id\", \"triangle_count\"):_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|Vertex_id|triangle_count|\n",
      "+---------+--------------+\n",
      "|       80|             1|\n",
      "|       30|             1|\n",
      "|       50|             0|\n",
      "|       40|             0|\n",
      "|       90|             1|\n",
      "|       70|             1|\n",
      "|       20|             1|\n",
      "|       60|             0|\n",
      "|       10|             1|\n",
      "+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tri_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msccGraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mVertexId\u001b[39m, \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@679f0259"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sccGraph = graph1.stronglyConnectedComponents(numIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mscc_df\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Vertex_id: bigint, strong_conn_comp: bigint]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scc_df = spark.createDataFrame(sccGraph.vertices).toDF(Seq(\"Vertex_id\", \"strong_conn_comp\"):_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|Vertex_id|strong_conn_comp|\n",
      "+---------+----------------+\n",
      "|       80|              70|\n",
      "|       30|              10|\n",
      "|       50|              50|\n",
      "|       40|              40|\n",
      "|       90|              70|\n",
      "|       70|              70|\n",
      "|       20|              10|\n",
      "|       60|              60|\n",
      "|       10|              10|\n",
      "+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scc_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (10,10) --[1.0]--> (20,10)\n",
      " (20,10) --[1.0]--> (30,10)\n",
      " (30,10) --[1.0]--> (10,10)\n",
      " (70,70) --[1.0]--> (80,70)\n",
      " (40,40) --[1.0]--> (50,50)\n",
      " (50,50) --[1.0]--> (60,60)\n",
      " (60,60) --[1.0]--> (20,10)\n",
      " (80,70) --[1.0]--> (90,70)\n",
      " (90,70) --[1.0]--> (70,70)\n"
     ]
    }
   ],
   "source": [
    "printOnlyEdges(sccGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex ID = 80: 70\n",
      "Vertex ID = 30: 10\n",
      "Vertex ID = 50: 50\n",
      "Vertex ID = 40: 40\n",
      "Vertex ID = 90: 70\n",
      "Vertex ID = 70: 70\n",
      "Vertex ID = 20: 10\n",
      "Vertex ID = 60: 60\n",
      "Vertex ID = 10: 10\n"
     ]
    }
   ],
   "source": [
    "printVertices(sccGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph1_vtx_data\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Vertex_id: bigint, Connected_id: bigint ... 3 more fields]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// join dataframes on Vertex_id : pgrank_df, cc_df, tri_df, scc_df\n",
    "val graph1_vtx_data = cc_df.join(pgrank_df,cc_df(\"Vertex_id\") === pgrank_df(\"Vertex_id\"),\"inner\" )\n",
    ".join(tri_df,cc_df(\"Vertex_id\") === tri_df(\"Vertex_id\"),\"inner\" )\n",
    ".join(scc_df,cc_df(\"Vertex_id\") === scc_df(\"Vertex_id\"),\"inner\" )\n",
    ".select(cc_df(\"Vertex_id\"),cc_df(\"Connected_id\"),pgrank_df(\"pagerank_score\"), tri_df(\"triangle_count\"), scc_df(\"strong_conn_comp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-------------------+--------------+----------------+\n",
      "|Vertex_id|Connected_id|     pagerank_score|triangle_count|strong_conn_comp|\n",
      "+---------+------------+-------------------+--------------+----------------+\n",
      "|       10|          10|  1.617229329338826|             1|              10|\n",
      "|       20|          10|  1.862052953709926|             1|              10|\n",
      "|       30|          10| 1.7297158594552773|             1|              10|\n",
      "|       40|          10|0.15569069912311598|             0|              40|\n",
      "|       50|          10| 0.2880277933777645|             0|              50|\n",
      "|       60|          10| 0.4005143234942158|             0|              60|\n",
      "|       70|          70| 0.9822563471669581|             1|              70|\n",
      "|       80|          70| 0.9822563471669581|             1|              70|\n",
      "|       90|          70| 0.9822563471669581|             1|              70|\n",
      "+---------+------------+-------------------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph1_vtx_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/22 15:24:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/05/22 15:24:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/05/22 15:24:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/05/22 15:24:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/05/22 15:24:11 INFO FileOutputCommitter: Saved output of task 'attempt_202305221524102744077725859891567_7408_m_000000_2480' to file:/work/src/spark_projs/graphx-algorithms/examples/graph1_properties.csv/_temporary/0/task_202305221524102744077725859891567_7408_m_000000\n"
     ]
    }
   ],
   "source": [
    "graph1_vtx_data.write.option(\"header\",true).mode(SaveMode.Overwrite).csv(\"graph1_properties.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Simple Example of using the Pregel API\n",
    "\n",
    "Pregel provides an iterative graph-parallel computation.\n",
    "\n",
    "Here is an example of computing the single source shortest path in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// val graph11 = graph1.pregel[A](initialMsg: A, maxIterations: Int, activeDirection: EdgeDirection)(\n",
    "//       vprog: (VertexId, VD, A) => VD,\n",
    "//       sendMsg: EdgeTriplet[VD, ED] => Iterator[(VertexId, A)],\n",
    "//       mergeMsg: (A, A) => A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mShortestPath\u001b[39m"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ShortestPath(graph: Graph[(Long, Double), Double], srcID: VertexId): RDD[Row] = {\n",
    "    \n",
    "    // Initialize the graph such that all vertices except the root have distance infinity.\n",
    "    val initialGraph = graph.mapVertices(\n",
    "        (id, _) =>\n",
    "        if (id == srcID) 0.0 else Double.PositiveInfinity\n",
    "    )\n",
    "\n",
    "    val sssp = initialGraph.pregel(Double.PositiveInfinity, maxIterations=7)(\n",
    "      (id, dist, newDist) => math.min(dist, newDist), // Vertex Program\n",
    "      triplet => {  // Send Message\n",
    "        if (triplet.srcAttr + triplet.attr < triplet.dstAttr) {\n",
    "          Iterator((triplet.dstId, triplet.srcAttr + triplet.attr))\n",
    "        } else {\n",
    "          Iterator.empty\n",
    "        }\n",
    "      },\n",
    "      (a, b) => math.min(a, b) // Merge Message\n",
    "    )\n",
    "    \n",
    "    return( sssp.vertices.filter(y => y._2 < Double.PositiveInfinity).map( x => Row(srcID, x._1, x._2)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdistances\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mRow\u001b[39m] = MapPartitionsRDD[4234] at map at cmd286.sc:21"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val distances = ShortestPath(graph1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrowArray\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mRow\u001b[39m] = \u001b[33mArray\u001b[39m([10,30,2.0], [10,20,1.0], [10,10,0.0])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rowArray = distances.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10,30,2.0]\n",
      "[10,20,1.0]\n",
      "[10,10,0.0]\n"
     ]
    }
   ],
   "source": [
    "println(distances.collect.mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Persisting Data to Storage\n",
    "\n",
    "All data and can be saved to disk.\n",
    "\n",
    "The default data format for saving to disk is Parquet which also compresses the data structure using SNAPPY compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/22 23:13:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/05/22 23:13:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/05/22 23:13:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/05/22 23:13:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/05/22 23:13:06 INFO FileOutputCommitter: Saved output of task 'attempt_202305222313068791303148318076102_4244_m_000000_0' to file:/work/src/spark_projs/graphx-algorithms/examples/vertices.obj/_temporary/0/task_202305222313068791303148318076102_4244_m_000000\n",
      "23/05/22 23:13:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/05/22 23:13:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/05/22 23:13:07 INFO FileOutputCommitter: Saved output of task 'attempt_202305222313068791303148318076102_4244_m_000001_0' to file:/work/src/spark_projs/graphx-algorithms/examples/vertices.obj/_temporary/0/task_202305222313068791303148318076102_4244_m_000001\n"
     ]
    }
   ],
   "source": [
    "// Save vertices RDD to disk\n",
    "graph1.vertices.saveAsObjectFile(\"vertices.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/22 23:13:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/05/22 23:13:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/05/22 23:13:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/05/22 23:13:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/05/22 23:13:13 INFO FileOutputCommitter: Saved output of task 'attempt_202305222313138990698911240321899_4246_m_000000_0' to file:/work/src/spark_projs/graphx-algorithms/examples/edges.obj/_temporary/0/task_202305222313138990698911240321899_4246_m_000000\n",
      "23/05/22 23:13:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/05/22 23:13:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/05/22 23:13:13 INFO FileOutputCommitter: Saved output of task 'attempt_202305222313138990698911240321899_4246_m_000001_0' to file:/work/src/spark_projs/graphx-algorithms/examples/edges.obj/_temporary/0/task_202305222313138990698911240321899_4246_m_000001\n"
     ]
    }
   ],
   "source": [
    "// then, save edges RDD to disk\n",
    "graph1.edges.saveAsObjectFile(\"edges.obj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Stop the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
